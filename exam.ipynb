{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# List of required packages (use package names as recognized by pip)\n",
    "required = {\n",
    "    'geopandas',\n",
    "    'osmnx',\n",
    "    'contextily',\n",
    "    'libpysal',\n",
    "    'esda',\n",
    "    'pointpats',\n",
    "    'matplotlib',\n",
    "    'seaborn',\n",
    "    'scikit-learn',\n",
    "    'geodatasets',\n",
    "    'folium'\n",
    "}\n",
    "\n",
    "# Get the set of installed packages\n",
    "installed = {pkg.key for pkg in pkg_resources.working_set}\n",
    "# Determine which packages are missing\n",
    "missing = required - installed\n",
    "\n",
    "if missing:\n",
    "    print(f\"Installing missing packages: {missing}\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", *missing])\n",
    "else:\n",
    "    print(\"All required packages are already installed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import osmnx as ox\n",
    "from shapely.geometry import Point, Polygon, shape\n",
    "from shapely.wkt import loads\n",
    "from libpysal.weights import Queen\n",
    "from esda import Moran, Moran_Local\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "import os\n",
    "import warnings\n",
    "from scipy.stats import gaussian_kde\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import contextily as ctx\n",
    "import libpysal\n",
    "import esda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this to get data from cityofnewyork.us\n",
    "def load_data(url, filename, usecols=None):\n",
    "    \"\"\"\n",
    "    Downloads a CSV file from a given URL and loads it into a DataFrame.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        print(f\"Downloading {filename}...\")\n",
    "        df = pd.read_csv(url, usecols=usecols)\n",
    "        df.to_csv(filename, index=False)\n",
    "    else:\n",
    "        print(f\"Loading {filename} from local file...\")\n",
    "        df = pd.read_csv(filename, usecols=usecols)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crime data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_data_path = \"./data/NYPD_Complaint_Data_Historic.csv\"\n",
    "crime_data_url = \"https://data.cityofnewyork.us/api/views/qgea-i56i/rows.csv?accessType=DOWNLOAD\"\n",
    "crime_cols = [\"CMPLNT_FR_DT\", \"LAW_CAT_CD\", \"BORO_NM\", \"ADDR_PCT_CD\", \"Latitude\", \"Longitude\"]\n",
    "\n",
    "crime_df = load_data(crime_data_url, crime_data_path, usecols=crime_cols)\n",
    "\n",
    "# Convert dates to datetime. Parse errors will set value to NaT\n",
    "crime_df[\"CMPLNT_FR_DT\"] = pd.to_datetime(crime_df[\"CMPLNT_FR_DT\"], format=\"%m/%d/%Y\", errors='coerce')\n",
    "\n",
    "# Filter for year 2019\n",
    "crime_df = crime_df[crime_df[\"CMPLNT_FR_DT\"].dt.year == 2019]\n",
    "\n",
    "# Drop records with missing or invalid coordinates\n",
    "crime_df = crime_df.dropna(subset=[\"Latitude\", \"Longitude\"])\n",
    "crime_df = crime_df[crime_df[\"Latitude\"] != 0]\n",
    "\n",
    "# convert to geodataframe\n",
    "crime_gdf = gpd.GeoDataFrame(\n",
    "    crime_df,\n",
    "    geometry=gpd.points_from_xy(crime_df[\"Longitude\"], crime_df[\"Latitude\"]),\n",
    "    crs=\"EPSG:4326\"\n",
    ").to_crs(epsg=3857)\n",
    "\n",
    "print(f\"Total records in 2019: {len(crime_df)}\")\n",
    "crime_gdf.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Population by Neighbourhood tabulation areas (NTA) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Load NTA polygons\n",
    "nta_polys_data_path = \"./data/NYC_NTA_Polygons.csv\"\n",
    "nta_polys_data_url  = \"https://data.cityofnewyork.us/api/views/9nt8-h7nd/rows.csv?accessType=DOWNLOAD\"\n",
    "nta_polys_df = load_data(nta_polys_data_url, nta_polys_data_path)\n",
    "\n",
    "# 2) Convert to GeoDataFrame (EPSG:4326 → EPSG:3857)\n",
    "nta_polys_gdf = gpd.GeoDataFrame(\n",
    "    nta_polys_df,\n",
    "    geometry=nta_polys_df[\"the_geom\"].apply(loads),\n",
    "    crs=\"EPSG:4326\"\n",
    ").to_crs(epsg=3857)\n",
    "\n",
    "# 3) Load NTA population table\n",
    "nta_pop_data_path = \"./data/NYC_NTA.csv\"\n",
    "nta_pop_data_url  = \"https://data.cityofnewyork.us/api/views/swpk-hqdp/rows.csv?accessType=DOWNLOAD\"\n",
    "nta_pop_df = load_data(nta_pop_data_url, nta_pop_data_path)\n",
    "\n",
    "# 4) Normalize population DataFrame columns\n",
    "nta_pop_df = nta_pop_df.rename(columns={\n",
    "    \"NTA Code\": \"NTA2020\",\n",
    "    \"Population\": \"population\",\n",
    "    \"NTA Name\": \"NTAName_pop\"\n",
    "})\n",
    "\n",
    "# 5) Merge on NTA2020 code\n",
    "nta_polys_gdf = nta_polys_gdf.merge(\n",
    "    nta_pop_df[[\"NTA2020\", \"population\"]],\n",
    "    on=\"NTA2020\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# 6) Fallback name‐based merge for any missing\n",
    "nta_pop_df[\"NTAName_lc\"]    = nta_pop_df[\"NTAName_pop\"].str.lower()\n",
    "nta_polys_gdf[\"NTAName_lc\"] = nta_polys_gdf[\"NTAName\"].str.lower()\n",
    "pop_lookup = nta_pop_df.set_index(\"NTAName_lc\")[\"population\"].to_dict()\n",
    "mask_missing = nta_polys_gdf[\"population\"].isna()\n",
    "nta_polys_gdf.loc[mask_missing, \"population\"] = (\n",
    "    nta_polys_gdf.loc[mask_missing, \"NTAName_lc\"].map(pop_lookup)\n",
    ")\n",
    "nta_polys_gdf = nta_polys_gdf.drop(columns=\"NTAName_lc\")\n",
    "\n",
    "from difflib import SequenceMatcher\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# 1) Identify NTAs still missing population\n",
    "unmatched = nta_polys_gdf[nta_polys_gdf['population'].isna()].copy()\n",
    "\n",
    "# 2) Build lookup: NTAName_pop → population\n",
    "name_to_pop = nta_pop_df.set_index('NTAName_pop')['population'].to_dict()\n",
    "pop_names   = list(name_to_pop.keys())\n",
    "\n",
    "# 3) Helper: common prefix length (case-insensitive)\n",
    "def common_prefix_len(a, b):\n",
    "    a, b = a.lower(), b.lower()\n",
    "    for i in range(min(len(a), len(b))):\n",
    "        if a[i] != b[i]:\n",
    "            return i\n",
    "    return min(len(a), len(b))\n",
    "\n",
    "# 4) Helper: longest substring match length\n",
    "def substring_match_len(a, b):\n",
    "    matcher = SequenceMatcher(None, a.lower(), b.lower())\n",
    "    match = matcher.find_longest_match(0, len(a), 0, len(b))\n",
    "    return match.size\n",
    "\n",
    "# 5) Compute both matches and their populations\n",
    "prefix_choices = []\n",
    "prefix_pops    = []\n",
    "substr_choices = []\n",
    "substr_pops    = []\n",
    "\n",
    "for name in unmatched['NTAName']:\n",
    "    # prefix-based\n",
    "    best_prefix = max(pop_names, key=lambda c: common_prefix_len(name, c))\n",
    "    prefix_choices.append(best_prefix)\n",
    "    prefix_pops.append(name_to_pop[best_prefix])\n",
    "    # substring-based\n",
    "    best_sub = max(pop_names, key=lambda c: substring_match_len(name, c))\n",
    "    substr_choices.append(best_sub)\n",
    "    substr_pops.append(name_to_pop[best_sub])\n",
    "\n",
    "unmatched['Closest Prefix Match']   = prefix_choices\n",
    "unmatched['Prefix Population']      = prefix_pops\n",
    "unmatched['Best Substring Match']   = substr_choices\n",
    "unmatched['Substring Population']   = substr_pops\n",
    "\n",
    "# 6) Prepare display table\n",
    "display_table = (\n",
    "    unmatched[['NTAName', \n",
    "               'Closest Prefix Match', 'Prefix Population',\n",
    "               'Best Substring Match', 'Substring Population']]\n",
    "    .rename(columns={'NTAName': 'Unmatched NTA Name'})\n",
    "    .sort_values('Unmatched NTA Name')\n",
    ")\n",
    "\n",
    "# 7) Display as Markdown\n",
    "display(Markdown(\n",
    "    \"## NTAs Missing Population with Closest Prefix & Substring Matches\\n\\n\"\n",
    "    + display_table.to_markdown(index=False)\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amenities data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amenities_data_path = \"./data/NYC_Amenities.csv\"\n",
    "\n",
    "# Function to extract coordinates from Point objects or calculate centroid for Polygons\n",
    "def extract_coordinates(geometry):\n",
    "    if isinstance(geometry, Point):\n",
    "        return geometry.x, geometry.y\n",
    "    elif isinstance(geometry, Polygon):\n",
    "        centroid = geometry.centroid\n",
    "        return centroid.x, centroid.y\n",
    "    return None, None\n",
    "\n",
    "amenities_df = None\n",
    "amenities_gdf = None\n",
    "\n",
    "# Apply the function to the dataset\n",
    "if os.path.exists(amenities_data_path):\n",
    "    print(\"Loading amenities data from local file...\")\n",
    "    amenities_df = pd.read_csv(amenities_data_path, low_memory=False)\n",
    "    # convert to geodataframe\n",
    "    amenities_gdf = gpd.GeoDataFrame(\n",
    "        amenities_df,\n",
    "        geometry=amenities_df[\"geometry\"].apply(loads),\n",
    "        crs=\"EPSG:4326\"\n",
    "    ).to_crs(epsg=3857)\n",
    "else:\n",
    "    print(\"Querying OSM for amenities data...\")\n",
    "    # Define a dictionary of tags for the amenities you're interested in\n",
    "    tags = {\n",
    "        \"amenity\": [\"bar\", \"restaurant\"],\n",
    "        \"leisure\": \"park\",\n",
    "        \"railway\": \"station\"\n",
    "    }\n",
    "\n",
    "    # Use OSMnx to query OSM for these features in New York City\n",
    "    amenities_gdf = ox.features.features_from_place(\"New York City, USA\", tags)\n",
    "\n",
    "    # Extract coordinates or calculate centroid\n",
    "    amenities_gdf[['Longitude', 'Latitude']] = amenities_gdf['geometry'].apply(lambda x: pd.Series(extract_coordinates(x)))\n",
    "\n",
    "    # Save the queried data to a CSV file for future use\n",
    "    amenities_gdf.to_csv(amenities_data_path, index=False)\n",
    "    amenities_df = amenities_gdf\n",
    "\n",
    "# Create a new column 'category' to combine 'leisure' and 'amenity'\n",
    "amenities_gdf['category'] = amenities_gdf['leisure'].combine_first(amenities_gdf['amenity']).combine_first(amenities_gdf['railway'])\n",
    "\n",
    "# 5) Filter to exactly the four types you want\n",
    "keep = [\"bar\", \"restaurant\", \"park\", \"station\"]\n",
    "amenities_gdf = amenities_gdf[amenities_gdf[\"category\"].isin(keep)]\n",
    "\n",
    "for amenity in amenities_gdf['category'].unique():\n",
    "    print(f\"Number of {amenity}: {len(amenities_gdf[amenities_gdf['category'] == amenity])}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VISUALIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "import numpy as np\n",
    "\n",
    "def preview_gdf(\n",
    "    gdf, \n",
    "    name, \n",
    "    groupby=None, \n",
    "    max_groups=None, \n",
    "    cols=None, \n",
    "    n=5, \n",
    "    random_state=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Preview a GeoDataFrame in Markdown:\n",
    "    - Reprojects to WGS84 (lat/lon).\n",
    "    - Computes 'Latitude'/'Longitude' from centroids.\n",
    "    - Always shows the 'geometry' column.\n",
    "    - If `groupby` is provided, samples 1 row per group (up to max_groups).\n",
    "      Otherwise, shows the first n rows.\n",
    "    - You can pass `cols` to include extra columns before geometry/lat/lon.\n",
    "    \"\"\"\n",
    "    # 1) Copy and reproject\n",
    "    df = gdf.copy().to_crs(epsg=4326)\n",
    "    # 2) Compute lat/lon\n",
    "    centroids = df.geometry.centroid\n",
    "    df['Latitude']  = centroids.y\n",
    "    df['Longitude'] = centroids.x\n",
    "\n",
    "    # 3) Decide rows to show\n",
    "    if groupby:\n",
    "        groups = df[groupby].dropna().unique()\n",
    "        if max_groups and len(groups) > max_groups:\n",
    "            rng = np.random.default_rng(random_state)\n",
    "            groups = rng.choice(groups, size=max_groups, replace=False)\n",
    "        df = (\n",
    "            df[df[groupby].isin(groups)]\n",
    "            .groupby(groupby, group_keys=False)\n",
    "            .apply(lambda sub: sub.sample(n=1, random_state=random_state))\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "        title = f\"{name} — Representative (1 per '{groupby}')\"\n",
    "    else:\n",
    "        df = df.head(n)\n",
    "        title = f\"{name} — Sample ({n} rows)\"\n",
    "\n",
    "    # 4) Build column list: user cols + geometry + lat/lon\n",
    "    display_cols = []\n",
    "    if cols:\n",
    "        display_cols += cols\n",
    "    # always include geometry\n",
    "    display_cols.append('geometry')\n",
    "    # then lat/lon\n",
    "    display_cols += ['Latitude', 'Longitude']\n",
    "\n",
    "    # 5) Render Markdown\n",
    "    md = df[display_cols].to_markdown(index=False)\n",
    "    display(Markdown(f\"## {title}\\n\\n{md}\"))\n",
    "\n",
    "def preview_df(\n",
    "    df, \n",
    "    name, \n",
    "    n=5\n",
    "):\n",
    "    \"\"\"\n",
    "    Preview a regular DataFrame in Markdown:\n",
    "    - Schema (column names & dtypes)\n",
    "    - First n rows\n",
    "    \"\"\"\n",
    "    schema = df.dtypes.reset_index()\n",
    "    schema.columns = ['column', 'dtype']\n",
    "    md_schema = schema.to_markdown(index=False)\n",
    "    md_sample = df.head(n).to_markdown(index=False)\n",
    "    display(Markdown(f\"## {name} — Schema\\n\\n{md_schema}\"))\n",
    "    display(Markdown(f\"## {name} — Sample ({n} rows)\\n\\n{md_sample}\"))\n",
    "\n",
    "# === Usage ===\n",
    "\n",
    "# 1) Crime: one per category\n",
    "crime_wgs = crime_gdf.to_crs(epsg=4326)\n",
    "preview_gdf(\n",
    "    crime_wgs,\n",
    "    \"Crime Data (2019)\",\n",
    "    groupby='LAW_CAT_CD',\n",
    "    max_groups=3,\n",
    "    cols=['CMPLNT_FR_DT', 'LAW_CAT_CD', 'BORO_NM']\n",
    ")\n",
    "\n",
    "# 2) Amenities: one per type\n",
    "amen_wgs = amenities_gdf.to_crs(epsg=4326)\n",
    "preview_gdf(\n",
    "    amen_wgs,\n",
    "    \"Amenities Data\",\n",
    "    groupby='category',\n",
    "    max_groups=4,\n",
    "    cols=['category', 'name']\n",
    ")\n",
    "\n",
    "# 3) NTA Polygons (with Population) — only sample from those with a real population\n",
    "nta_wgs = nta_polys_gdf.to_crs(epsg=4326)\n",
    "\n",
    "# filter out any NTAs still missing population\n",
    "nta_with_pop = nta_wgs[nta_wgs['population'].notna()]\n",
    "\n",
    "# pick exactly one representative row (with a real population)\n",
    "preview_gdf(\n",
    "    nta_with_pop,\n",
    "    \"NTA Polygons (with Population)\",\n",
    "    groupby='NTA2020',\n",
    "    max_groups=1,\n",
    "    cols=['NTA2020', 'NTAName', 'BoroName', 'population']\n",
    ")\n",
    "\n",
    "# 4) NTA population table (regular DataFrame)\n",
    "preview_df(\n",
    "    nta_pop_df,\n",
    "    \"NTA Population Data\",\n",
    "    n=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from shapely.ops import unary_union\n",
    "from folium.plugins import HeatMap, MarkerCluster, GroupedLayerControl\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "USE_SAMPLE      = True      # If True, sample a fraction of points for speed\n",
    "SAMPLE_FRACTION = 0.03      # Fraction to sample when USE_SAMPLE=True\n",
    "RANDOM_STATE    = 42\n",
    "\n",
    "# === 1) Prepare GeoDataFrames ===\n",
    "# Assumes crime_gdf, amenities_gdf, nta_polys_gdf are pre-loaded\n",
    "\n",
    "# Reproject all to WGS84 (latitude/longitude) for mapping\n",
    "crime     = crime_gdf.to_crs(epsg=4326).copy()\n",
    "amenities = amenities_gdf.to_crs(epsg=4326).copy()\n",
    "ntas      = nta_polys_gdf.to_crs(epsg=4326).copy()\n",
    "\n",
    "# Also prepare Web Mercator for distance-in-meters computations\n",
    "crime_m     = crime.to_crs(epsg=3857).copy()\n",
    "amenities_m = amenities.to_crs(epsg=3857).copy()\n",
    "ntas_m      = ntas.to_crs(epsg=3857).copy()\n",
    "\n",
    "# === 2) (Optional) Sample for performance ===\n",
    "if USE_SAMPLE:\n",
    "    crime     = crime.sample(frac=SAMPLE_FRACTION, random_state=RANDOM_STATE)\n",
    "    crime_m   = crime_m.loc[crime.index]\n",
    "    amenities = amenities.sample(frac=SAMPLE_FRACTION, random_state=RANDOM_STATE)\n",
    "    amenities_m = amenities_m.loc[amenities.index]\n",
    "\n",
    "# === 3) Extract latitude and longitude for Folium ===\n",
    "crime['latitude']   = crime.geometry.y\n",
    "crime['longitude']  = crime.geometry.x\n",
    "amen_centroids      = amenities.geometry.centroid\n",
    "amenities['latitude']  = amen_centroids.y\n",
    "amenities['longitude'] = amen_centroids.x\n",
    "\n",
    "# === 4) Count crimes and amenities per NTA ===\n",
    "crime_ntas = gpd.sjoin(\n",
    "    crime[['geometry']],\n",
    "    ntas[['NTA2020','population','geometry']],\n",
    "    how='inner', predicate='within'\n",
    ")\n",
    "crime_counts = crime_ntas.groupby('NTA2020').size().rename('crime_count')\n",
    "\n",
    "amen_ntas = gpd.sjoin(\n",
    "    amenities[['geometry']],\n",
    "    ntas[['NTA2020','population','geometry']],\n",
    "    how='inner', predicate='within'\n",
    ")\n",
    "amenity_counts = amen_ntas.groupby('NTA2020').size().rename('amenity_count')\n",
    "\n",
    "ntas = (\n",
    "    ntas.set_index('NTA2020')\n",
    "        .join(crime_counts, how='left')\n",
    "        .join(amenity_counts, how='left')\n",
    "        .fillna({'crime_count': 0, 'amenity_count': 0})\n",
    "        .reset_index()\n",
    ")\n",
    "ntas['crime_rate']             = ntas['crime_count']            / ntas['population']\n",
    "ntas['amenity_per_capita']     = ntas['amenity_count']          / ntas['population']\n",
    "ntas['crime_to_amenity_ratio'] = ntas['crime_count']            / ntas['amenity_count'].replace({0: np.nan})\n",
    "\n",
    "# === 5) Aggregate metrics per borough ===\n",
    "boroughs = (\n",
    "    ntas[['BoroName','population','crime_count','amenity_count','geometry']]\n",
    "    .dissolve(by='BoroName', aggfunc='sum')\n",
    "    .reset_index()\n",
    ")\n",
    "boroughs['density']           = boroughs['population'] / boroughs.geometry.to_crs(epsg=3857).area\n",
    "boroughs['crime_rate_boro']   = boroughs['crime_count']   / boroughs['population']\n",
    "boroughs['amenity_rate_boro'] = boroughs['amenity_count'] / boroughs['population']\n",
    "\n",
    "# === 6) Pre‐compute MultiPoint unions for each amenity type ===\n",
    "unions = {}\n",
    "for amen_type in ['bar','restaurant','park','station']:\n",
    "    pts = amenities_m[amenities_m['category'] == amen_type].geometry\n",
    "    unions[amen_type] = unary_union(pts)\n",
    "\n",
    "# === 7) Compute nearest‐amenity distance for each crime point ===\n",
    "for amen_type, union_geom in unions.items():\n",
    "    # distance in meters to the single closest amenity of that type\n",
    "    crime_m[f'dist_to_{amen_type}'] = crime_m.geometry.distance(union_geom)\n",
    "    crime[f'dist_to_{amen_type}']   = crime_m.loc[crime.index, f'dist_to_{amen_type}']\n",
    "\n",
    "# === 8) Compute average distances per NTA and per borough ===\n",
    "distance_fields = ['dist_to_bar','dist_to_restaurant','dist_to_park','dist_to_station']\n",
    "\n",
    "# Average per NTA\n",
    "c2n = gpd.sjoin(\n",
    "    crime[['geometry'] + distance_fields],\n",
    "    ntas[['NTA2020','geometry']],\n",
    "    how='inner', predicate='within'\n",
    ")\n",
    "dist_ntas = (\n",
    "    c2n.groupby('NTA2020')[distance_fields]\n",
    "       .mean()\n",
    "       .rename(columns={f: f'avg_{f}' for f in distance_fields})\n",
    ")\n",
    "ntas = ntas.set_index('NTA2020').join(dist_ntas).reset_index()\n",
    "\n",
    "# Average per borough\n",
    "c2b = gpd.sjoin(\n",
    "    crime[['geometry'] + distance_fields],\n",
    "    boroughs[['BoroName','geometry']],\n",
    "    how='inner', predicate='within'\n",
    ")\n",
    "dist_boros = (\n",
    "    c2b.groupby('BoroName')[distance_fields]\n",
    "       .mean()\n",
    "       .rename(columns={f: f'avg_{f}' for f in distance_fields})\n",
    ")\n",
    "boroughs = boroughs.set_index('BoroName').join(dist_boros).reset_index()\n",
    "\n",
    "# === 9) Initialize Folium map ===\n",
    "center = [crime['latitude'].mean(), crime['longitude'].mean()]\n",
    "m = folium.Map(location=center, zoom_start=11, tiles=None)\n",
    "folium.TileLayer('CartoDB Positron', name='Basemap', control=True).add_to(m)\n",
    "\n",
    "# === 10) Crime point and cluster layers ===\n",
    "crime_point_layers = []\n",
    "crime_cluster_layers = []\n",
    "for category, color in [('Felony','crimson'),('Misdemeanor','orange'),('Violation','blue')]:\n",
    "    # Individual points\n",
    "    fg_pts = folium.FeatureGroup(name=f\"Crime: {category}\", show=False)\n",
    "    subset = crime[crime['LAW_CAT_CD'].str.title() == category]\n",
    "    for _, r in subset.iterrows():\n",
    "        folium.CircleMarker(\n",
    "            [r['latitude'], r['longitude']],\n",
    "            radius=3, color=color, fill=True, fill_opacity=0.6\n",
    "        ).add_to(fg_pts)\n",
    "    fg_pts.add_to(m)\n",
    "    crime_point_layers.append(fg_pts)\n",
    "\n",
    "    # Cluster markers\n",
    "    fg_cl = MarkerCluster(name=f\"Crime Clusters: {category}\", show=False)\n",
    "    for _, r in subset.iterrows():\n",
    "        folium.Marker([r['latitude'], r['longitude']]).add_to(fg_cl)\n",
    "    fg_cl.add_to(m)\n",
    "    crime_cluster_layers.append(fg_cl)\n",
    "\n",
    "# === 11) Crime heatmap ===\n",
    "crime_heat = folium.FeatureGroup(name=\"Crime Heatmap\", show=False)\n",
    "HeatMap(\n",
    "    list(zip(crime['latitude'], crime['longitude'])),\n",
    "    radius=15, blur=10, min_opacity=0.3\n",
    ").add_to(crime_heat)\n",
    "crime_heat.add_to(m)\n",
    "\n",
    "# === 12) Amenity point and cluster layers ===\n",
    "amenity_point_layers = []\n",
    "amenity_cluster_layers = []\n",
    "for amen, color in [('Bar','purple'),('Restaurant','darkgreen'),('Park','green'),('Station','cadetblue')]:\n",
    "    # Small circle markers\n",
    "    fg_pts = folium.FeatureGroup(name=f\"Amenity: {amen}\", show=False)\n",
    "    mc = MarkerCluster().add_to(fg_pts)\n",
    "    subset = amenities[amenities['category'].str.title() == amen]\n",
    "    for _, r in subset.iterrows():\n",
    "        folium.CircleMarker(\n",
    "            [r['latitude'], r['longitude']],\n",
    "            radius=3, color=color, fill=True, fill_opacity=0.6\n",
    "        ).add_to(mc)\n",
    "    fg_pts.add_to(m)\n",
    "    amenity_point_layers.append(fg_pts)\n",
    "\n",
    "    # Clustered raw markers\n",
    "    fg_cl = MarkerCluster(name=f\"Amenity Clusters: {amen}\", show=False)\n",
    "    for _, r in subset.iterrows():\n",
    "        folium.Marker([r['latitude'], r['longitude']]).add_to(fg_cl)\n",
    "    fg_cl.add_to(m)\n",
    "    amenity_cluster_layers.append(fg_cl)\n",
    "\n",
    "# === 13) Amenity heatmap ===\n",
    "amen_heat = folium.FeatureGroup(name=\"Amenity Heatmap (All)\", show=False)\n",
    "HeatMap(\n",
    "    list(zip(amenities['latitude'], amenities['longitude'])),\n",
    "    radius=15, blur=10, min_opacity=0.3\n",
    ").add_to(amen_heat)\n",
    "amen_heat.add_to(m)\n",
    "\n",
    "# === 14) Choropleth style helper ===\n",
    "def make_choro_style(series):\n",
    "    cuts = series.quantile([0.2,0.4,0.6,0.8]).values\n",
    "    def style_fn(feature):\n",
    "        v = feature['properties'].get(series.name)\n",
    "        if v is None or np.isnan(v):\n",
    "            c = 'grey'\n",
    "        elif v > cuts[3]:\n",
    "            c = 'red'\n",
    "        elif v > cuts[2]:\n",
    "            c = 'orange'\n",
    "        elif v > cuts[1]:\n",
    "            c = 'yellow'\n",
    "        elif v > cuts[0]:\n",
    "            c = 'lightgreen'\n",
    "        else:\n",
    "            c = 'green'\n",
    "        return {'fillColor': c, 'color': 'black', 'weight': 1, 'fillOpacity': 0.6}\n",
    "    return style_fn\n",
    "\n",
    "# === 15) NTA metric choropleths ===\n",
    "nta_layers = []\n",
    "for title, field, aliases in [\n",
    "    (\"Crime Rate\",         'crime_rate',             ['Neighborhood','Crime Rate']),\n",
    "    (\"Amenities/Capita\",   'amenity_per_capita',     ['Neighborhood','Amenities/Capita']),\n",
    "    (\"Crime/Amenity Ratio\",'crime_to_amenity_ratio',['Neighborhood','Crime/Amenity'])\n",
    "]:\n",
    "    fg = folium.FeatureGroup(name=f\"NTA: {title}\", show=False)\n",
    "    folium.GeoJson(\n",
    "        ntas,\n",
    "        style_function=make_choro_style(ntas[field]),\n",
    "        tooltip=folium.GeoJsonTooltip(fields=['NTAName', field], aliases=aliases)\n",
    "    ).add_to(fg)\n",
    "    fg.add_to(m)\n",
    "    nta_layers.append(fg)\n",
    "\n",
    "# === 16) Borough metric choropleths ===\n",
    "boro_layers = []\n",
    "for title, field, aliases in [\n",
    "    (\"Population Density\",'density',           ['Borough','Density']),\n",
    "    (\"Crime Rate\",       'crime_rate_boro',   ['Borough','Crime Rate']),\n",
    "    (\"Amenities Rate\",   'amenity_rate_boro', ['Borough','Amenities Rate'])\n",
    "]:\n",
    "    fg = folium.FeatureGroup(name=f\"Borough: {title}\", show=False)\n",
    "    folium.GeoJson(\n",
    "        boroughs,\n",
    "        style_function=make_choro_style(boroughs[field]),\n",
    "        tooltip=folium.GeoJsonTooltip(fields=['BoroName', field], aliases=aliases)\n",
    "    ).add_to(fg)\n",
    "    # Label boroughs\n",
    "    for _, r in boroughs.iterrows():\n",
    "        folium.map.Marker(\n",
    "            [r.geometry.centroid.y, r.geometry.centroid.x],\n",
    "            icon=folium.DivIcon(html=f\"<div style='font-size:12px'><b>{r['BoroName']}</b></div>\")\n",
    "        ).add_to(fg)\n",
    "    fg.add_to(m)\n",
    "    boro_layers.append(fg)\n",
    "\n",
    "# === 17) NTA average distance to each amenity type ===\n",
    "distance_layers = []\n",
    "for amen_type in ['bar','restaurant','park','station']:\n",
    "    field = f'avg_dist_to_{amen_type}'\n",
    "    fg = folium.FeatureGroup(name=f\"NTA Avg Distance to {amen_type.title()} (m)\", show=False)\n",
    "    folium.GeoJson(\n",
    "        ntas,\n",
    "        style_function=make_choro_style(ntas[field]),\n",
    "        tooltip=folium.GeoJsonTooltip(\n",
    "            fields=['NTAName', field],\n",
    "            aliases=['Neighborhood', f'Avg m to {amen_type.title()}']\n",
    "        )\n",
    "    ).add_to(fg)\n",
    "    # Numeric labels\n",
    "    for _, r in ntas.iterrows():\n",
    "        val = r[field]\n",
    "        if not np.isnan(val):\n",
    "            folium.map.Marker(\n",
    "                [r.geometry.centroid.y, r.geometry.centroid.x],\n",
    "                icon=folium.DivIcon(html=f\"<div style='font-size:8px;color:black'>{int(val)}</div>\")\n",
    "            ).add_to(fg)\n",
    "    fg.add_to(m)\n",
    "    distance_layers.append(fg)\n",
    "\n",
    "# === 18) Crime→Amenity Distances by Type ===\n",
    "combo_layers = []\n",
    "for crime_cat in ['Felony','Misdemeanor','Violation']:\n",
    "    for amen_type in ['bar','restaurant','park','station']:\n",
    "        field = f'avg_dist_{crime_cat.lower()}_to_{amen_type}'\n",
    "        # Compute mean nearest distance per NTA\n",
    "        subset = crime[crime['LAW_CAT_CD'].str.title() == crime_cat]\n",
    "        c2n_cat = gpd.sjoin(\n",
    "            subset[['geometry', f'dist_to_{amen_type}']],\n",
    "            ntas[['NTA2020','geometry']],\n",
    "            how='inner', predicate='within'\n",
    "        )\n",
    "        avg_dist = c2n_cat.groupby('NTA2020')[f'dist_to_{amen_type}'].mean()\n",
    "        ntas[field] = ntas['NTA2020'].map(avg_dist)\n",
    "\n",
    "        explanation = (\n",
    "            f\"Average over all {crime_cat.lower()} incidents of the \"\n",
    "            f\"distance (in meters) to the single closest {amen_type}\"\n",
    "        )\n",
    "        layer_name = (\n",
    "            f\"NTA {crime_cat}→{amen_type.title()} Distance \"\n",
    "            f\"(*{explanation}*)\"\n",
    "        )\n",
    "        fg = folium.FeatureGroup(name=layer_name, show=False)\n",
    "        folium.GeoJson(\n",
    "            ntas,\n",
    "            style_function=make_choro_style(ntas[field]),\n",
    "            tooltip=folium.GeoJsonTooltip(\n",
    "                fields=['NTAName', field],\n",
    "                aliases=['Neighborhood', f'{crime_cat}→{amen_type.title()} (m)']\n",
    "            )\n",
    "        ).add_to(fg)\n",
    "        # Value labels\n",
    "        for _, r in ntas.iterrows():\n",
    "            val = r[field]\n",
    "            if not np.isnan(val):\n",
    "                folium.map.Marker(\n",
    "                    [r.geometry.centroid.y, r.geometry.centroid.x],\n",
    "                    icon=folium.DivIcon(html=f\"<div style='font-size:7px'>{int(val)}</div>\")\n",
    "                ).add_to(fg)\n",
    "        fg.add_to(m)\n",
    "        combo_layers.append(fg)\n",
    "\n",
    "# === 19) Grouped Layer Control ===\n",
    "GroupedLayerControl(\n",
    "    groups={\n",
    "        'Crime Points':                    crime_point_layers,\n",
    "        'Crime Clusters':                  crime_cluster_layers,\n",
    "        'Crime Heatmap':                   [crime_heat],\n",
    "        'Amenities Points':                amenity_point_layers,\n",
    "        'Amenity Clusters':                amenity_cluster_layers,\n",
    "        'Amenity Heatmap':                 [amen_heat],\n",
    "        'NTA Metrics':                     nta_layers,\n",
    "        'Borough Metrics':                 boro_layers,\n",
    "        'NTA Distances to Amenities':      distance_layers,\n",
    "        'Crime→Amenity Distances by Type': combo_layers\n",
    "    },\n",
    "    exclusive_groups=[],    # allow multiple on/off\n",
    "    collapse=False,\n",
    "    position='topright'\n",
    ").add_to(m)\n",
    "\n",
    "# === 20) Display the map ===\n",
    "m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brugbar tekst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## god kilde\n",
    "\n",
    "Urban crime patterns have been studied from both social and spatial perspectives. Criminology theories such as Broken Windows (Wilson & Kelling, 1982) suggest that the environment plays a role in crime prevalence – for example, disorder in the physical environment might encourage criminal behavior. On the other hand, urbanist Jane Jacobs (1961) argued that active streetscapes with plenty of “eyes on the street” can deter crime, implying that amenities attracting people (cafés, bars, etc.) might enhance safety through informal surveillance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crime Hotspots (Clustering Analysis) - DBSCAN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DBSCAN clustering to identify hotspots ---\n",
    "coords = np.column_stack([crime_gdf.geometry.x, crime_gdf.geometry.y])\n",
    "db = DBSCAN(eps=500, min_samples=30).fit(coords)\n",
    "crime_gdf[\"cluster\"] = db.labels_\n",
    "\n",
    "# Identify top 5 largest clusters (exclude noise)\n",
    "labels = crime_gdf[\"cluster\"].values\n",
    "clusters = [lab for lab in set(labels) if lab != -1]\n",
    "clusters.sort(key=lambda c: (labels == c).sum(), reverse=True)\n",
    "top5 = clusters[:5]\n",
    "print(\"Top 5 clusters and sizes:\", {c: int((labels == c).sum()) for c in top5})\n",
    "\n",
    "# Build convex hulls for these clusters\n",
    "hotspot_hulls = []\n",
    "for c in top5:\n",
    "    pts = crime_gdf[crime_gdf[\"cluster\"] == c]\n",
    "    hull = pts.unary_union.convex_hull\n",
    "    hotspot_hulls.append({\n",
    "        \"cluster\": c,\n",
    "        \"count\": int((labels == c).sum()),\n",
    "        \"geometry\": hull\n",
    "    })\n",
    "hotspots_gdf = gpd.GeoDataFrame(hotspot_hulls, crs=crime_gdf.crs)\n",
    "\n",
    "# Ensure Web Mercator for contextily\n",
    "hotspots_gdf = hotspots_gdf.to_crs(epsg=3857)\n",
    "\n",
    "# --- Plotting ---\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "# sample crime points for context\n",
    "crime_gdf.sample(frac=0.1).plot(ax=ax, color=\"grey\", alpha=0.1, markersize=1)\n",
    "\n",
    "# plot hull boundaries and fills\n",
    "hotspots_gdf.boundary.plot(\n",
    "    ax=ax, color='red', linewidth=2, linestyle='--'\n",
    ")\n",
    "hotspots_gdf.plot(\n",
    "    ax=ax, column='cluster', alpha=0.3, cmap='Set1'\n",
    ")\n",
    "\n",
    "# add basemap with fallback\n",
    "try:\n",
    "    ctx.add_basemap(\n",
    "        ax,\n",
    "        source=ctx.providers.OpenStreetMap.Mapnik,\n",
    "        crs=crime_gdf.crs\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"⚠️ Basemap failed:\", e)\n",
    "\n",
    "ax.set_title(\n",
    "    \"Figure 2. Top 5 Crime Hotspots in NYC (2019)\", fontsize=15\n",
    ")\n",
    "ax.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AGAIN USE NTA's instead of boroughs\n",
    "\n",
    "# 1) Ensure boroughs has the necessary columns\n",
    "boroughs['boro_name'] = boroughs.get('boro_name', boroughs['BoroName'])\n",
    "boroughs['crime_count'] = boroughs['crime_count'].fillna(0).astype(int)\n",
    "\n",
    "# 2) Build Queen contiguity weights and row-standardize\n",
    "w = libpysal.weights.Queen.from_dataframe(boroughs)\n",
    "w.transform = 'R'\n",
    "\n",
    "# 3) Extract crime counts and compute spatial lag (neighbor average)\n",
    "counts = boroughs['crime_count'].values\n",
    "lag = libpysal.weights.lag_spatial(w, counts)\n",
    "\n",
    "# 4) Print a summary table\n",
    "print(f\"{'Borough':<20} {'Crime':>6} {'Neighbor Avg':>14}\")\n",
    "print(\"-\" * 42)\n",
    "for name, cnt, nbr_avg in zip(boroughs['boro_name'], counts, lag):\n",
    "    print(f\"{name:<20} {cnt:6d} {nbr_avg:14.1f}\")\n",
    "\n",
    "# 5) Compute Global Moran's I\n",
    "mi = esda.Moran(counts, w)\n",
    "print(f\"\\nGlobal Moran's I: {mi.I:.3f}\")\n",
    "print(f\"p-value:           {mi.p_sim:.3f}\")\n",
    "\n",
    "# 6) Add the spatial lag back to the GeoDataFrame\n",
    "boroughs['spatial_lag'] = lag\n",
    "\n",
    "# 7) Choropleth maps side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 7))\n",
    "\n",
    "boroughs.plot(\n",
    "    column='crime_count',\n",
    "    cmap='Reds',\n",
    "    legend=True,\n",
    "    ax=axes[0],\n",
    "    edgecolor='black'\n",
    ")\n",
    "axes[0].set_title('Crime Count by Borough (2019)')\n",
    "axes[0].axis('off')\n",
    "\n",
    "boroughs.plot(\n",
    "    column='spatial_lag',\n",
    "    cmap='Blues',\n",
    "    legend=True,\n",
    "    ax=axes[1],\n",
    "    edgecolor='black'\n",
    ")\n",
    "axes[1].set_title('Spatial Lag of Crime Count')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 8) Moran’s I scatterplot\n",
    "y     = (counts      - counts.mean())      / counts.std()\n",
    "y_lag = (lag - lag.mean()) / lag.std()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "ax.axhline(0, color='gray', linewidth=1)\n",
    "ax.axvline(0, color='gray', linewidth=1)\n",
    "ax.scatter(y, y_lag, s=100, color='steelblue')\n",
    "\n",
    "for i, name in enumerate(boroughs['boro_name']):\n",
    "    ax.text(y[i] + 0.02, y_lag[i] + 0.02, name, fontsize=9)\n",
    "\n",
    "ax.set_xlabel('Standardized Crime Count')\n",
    "ax.set_ylabel('Standardized Spatial Lag')\n",
    "ax.set_title(\"Moran's I Scatterplot\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Count restaurants per borough via spatial join\n",
    "rests = amenities_gdf[amenities_gdf[\"category\"] == \"restaurant\"]\n",
    "rests_in_boro = gpd.sjoin(\n",
    "    rests,\n",
    "    boroughs[[\"boro_name\", \"geometry\"]],\n",
    "    how=\"inner\",\n",
    "    predicate=\"within\"\n",
    ")\n",
    "rest_counts = rests_in_boro.groupby(\"boro_name\").size().to_dict()\n",
    "\n",
    "# 2) Add restaurant counts to boroughs GeoDataFrame\n",
    "boroughs[\"rest_count\"] = boroughs[\"boro_name\"].map(rest_counts).fillna(0).astype(int)\n",
    "\n",
    "# 3) Build feature matrix [crime_count, rest_count] and normalize\n",
    "features = boroughs[[\"crime_count\", \"rest_count\"]].values\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(features)\n",
    "\n",
    "# 4) Fit KMeans (2 clusters) and attach labels\n",
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n",
    "boroughs[\"cluster\"] = kmeans.labels_\n",
    "\n",
    "# 5) Print results\n",
    "for _, row in boroughs.iterrows():\n",
    "    print(\n",
    "        f\"{row.boro_name}: Crime={row.crime_count}, \"\n",
    "        f\"Restaurants={row.rest_count}, Cluster={row.cluster}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Prepare coordinates of all crime points (in Web Mercator meters)\n",
    "coords = np.column_stack([crime_gdf.geometry.x, crime_gdf.geometry.y])\n",
    "\n",
    "# 2. Compute nearest‐neighbor distances (skip self–distance)\n",
    "nn = NearestNeighbors(n_neighbors=2, metric='euclidean').fit(coords)\n",
    "distances, _ = nn.kneighbors(coords)\n",
    "nn_distances = distances[:, 1]  # true nearest neighbor\n",
    "\n",
    "# 3. Calculate observed Average Nearest Neighbor (ANN)\n",
    "ann = nn_distances.mean()\n",
    "\n",
    "# 4. Calculate expected ANN under Complete Spatial Randomness (CSR)\n",
    "area = boroughs.geometry.unary_union.area     # total study area in m²\n",
    "n_points = len(crime_gdf)                     # number of crime incidents\n",
    "expected_ann = 0.5 * np.sqrt(area / n_points)\n",
    "\n",
    "# 5. Calculate the R ratio\n",
    "R = ann / expected_ann\n",
    "\n",
    "# 6. Beginner‐friendly printout\n",
    "print(\"=== Average Nearest Neighbor (ANN) Analysis ===\")\n",
    "print(f\"Observed ANN: {ann:.2f} m\")\n",
    "print(f\"Expected ANN (CSR): {expected_ann:.2f} m\")\n",
    "print(f\"R ratio: {R:.3f}\\n\")\n",
    "print(\"Interpretation:\")\n",
    "if R < 1:\n",
    "    print(f\"  • R = {R:.3f} < 1 → points are clustered (closer than random).\")\n",
    "elif R > 1:\n",
    "    print(f\"  • R = {R:.3f} > 1 → points are dispersed (farther than random).\")\n",
    "else:\n",
    "    print(f\"  • R = {R:.3f} ≈ 1 → random spatial pattern.\")\n",
    "print(f\"On average, each crime is {ann:.2f} m from its nearest neighbor,\")\n",
    "print(f\"much smaller than the {expected_ann:.2f} m we'd expect if crimes were random.\\n\")\n",
    "\n",
    "# 7. Visualization: histogram of nearest‐neighbor distances\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(nn_distances, bins=50, edgecolor='black')\n",
    "plt.axvline(ann, color='red', linestyle='--', linewidth=2, label=f'Observed ANN ({ann:.2f} m)')\n",
    "plt.axvline(expected_ann, color='blue', linestyle=':', linewidth=2, label=f'Expected ANN ({expected_ann:.2f} m)')\n",
    "plt.title(\"Histogram of Nearest‐Neighbor Distances (Crime Incidents)\")\n",
    "plt.xlabel(\"Distance to Nearest Crime (meters)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
