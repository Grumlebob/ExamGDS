{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# List of required packages (use package names as recognized by pip)\n",
    "required = {\n",
    "    'geopandas',\n",
    "    'osmnx',\n",
    "    'contextily',\n",
    "    'libpysal',\n",
    "    'esda',\n",
    "    'pointpats',\n",
    "    'matplotlib',\n",
    "    'seaborn',\n",
    "    'scikit-learn',\n",
    "    'geodatasets',\n",
    "    'folium'\n",
    "}\n",
    "\n",
    "# Get the set of installed packages\n",
    "installed = {pkg.key for pkg in pkg_resources.working_set}\n",
    "# Determine which packages are missing\n",
    "missing = required - installed\n",
    "\n",
    "if missing:\n",
    "    print(f\"Installing missing packages: {missing}\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", *missing])\n",
    "else:\n",
    "    print(\"All required packages are already installed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import osmnx as ox\n",
    "from shapely.geometry import Point, Polygon, shape\n",
    "from shapely.wkt import loads\n",
    "from libpysal.weights import Queen\n",
    "from esda import Moran, Moran_Local\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "import os\n",
    "import warnings\n",
    "from scipy.stats import gaussian_kde\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import contextily as ctx\n",
    "import libpysal\n",
    "import esda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this to get data from cityofnewyork.us\n",
    "def load_data(url, filename, usecols=None):\n",
    "    \"\"\"\n",
    "    Downloads a CSV file from a given URL and loads it into a DataFrame.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        print(f\"Downloading {filename}...\")\n",
    "        df = pd.read_csv(url, usecols=usecols)\n",
    "        df.to_csv(filename, index=False)\n",
    "    else:\n",
    "        print(f\"Loading {filename} from local file...\")\n",
    "        df = pd.read_csv(filename, usecols=usecols)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crime data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_data_path = \"./data/NYPD_Complaint_Data_Historic.csv\"\n",
    "crime_data_url = \"https://data.cityofnewyork.us/api/views/qgea-i56i/rows.csv?accessType=DOWNLOAD\"\n",
    "crime_cols = [\"CMPLNT_FR_DT\", \"LAW_CAT_CD\", \"BORO_NM\", \"ADDR_PCT_CD\", \"Latitude\", \"Longitude\"]\n",
    "\n",
    "crime_df = load_data(crime_data_url, crime_data_path, usecols=crime_cols)\n",
    "\n",
    "# Convert dates to datetime. Parse errors will set value to NaT\n",
    "crime_df[\"CMPLNT_FR_DT\"] = pd.to_datetime(crime_df[\"CMPLNT_FR_DT\"], format=\"%m/%d/%Y\", errors='coerce')\n",
    "\n",
    "# Filter for year 2019\n",
    "crime_df = crime_df[crime_df[\"CMPLNT_FR_DT\"].dt.year == 2019]\n",
    "\n",
    "# Drop records with missing or invalid coordinates\n",
    "crime_df = crime_df.dropna(subset=[\"Latitude\", \"Longitude\"])\n",
    "crime_df = crime_df[crime_df[\"Latitude\"] != 0]\n",
    "\n",
    "# convert to geodataframe\n",
    "crime_gdf = gpd.GeoDataFrame(\n",
    "    crime_df,\n",
    "    geometry=gpd.points_from_xy(crime_df[\"Longitude\"], crime_df[\"Latitude\"]),\n",
    "    crs=\"EPSG:4326\"\n",
    ").to_crs(epsg=3857)\n",
    "\n",
    "print(f\"Total records in 2019: {len(crime_df)}\")\n",
    "crime_gdf.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Population by Neighbourhood tabulation areas (NTA) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import re\n",
    "from shapely.wkt import loads\n",
    "from difflib import SequenceMatcher\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# ——————————————————————————————————————————————————————————————\n",
    "# 0) Assumes `load_data(url, path)` is defined elsewhere\n",
    "# ——————————————————————————————————————————————————————————————\n",
    "\n",
    "# 1) Load NTA polygons\n",
    "nta_polys_data_path = \"./data/NYC_NTA_Polygons.csv\"\n",
    "nta_polys_data_url  = \"https://data.cityofnewyork.us/api/views/9nt8-h7nd/rows.csv?accessType=DOWNLOAD\"\n",
    "nta_polys_df = load_data(nta_polys_data_url, nta_polys_data_path)\n",
    "\n",
    "# 2) Convert to GeoDataFrame (EPSG:4326 → EPSG:3857)\n",
    "nta_polys_gdf = gpd.GeoDataFrame(\n",
    "    nta_polys_df,\n",
    "    geometry=nta_polys_df[\"the_geom\"].apply(loads),\n",
    "    crs=\"EPSG:4326\"\n",
    ").to_crs(epsg=3857)\n",
    "\n",
    "# 3) Load NTA population table\n",
    "nta_pop_data_path = \"./data/NYC_NTA.csv\"\n",
    "nta_pop_data_url  = \"https://data.cityofnewyork.us/api/views/swpk-hqdp/rows.csv?accessType=DOWNLOAD\"\n",
    "nta_pop_df = load_data(nta_pop_data_url, nta_pop_data_path)\n",
    "\n",
    "# 4) Normalize population DataFrame columns\n",
    "nta_pop_df = nta_pop_df.rename(columns={\n",
    "    \"NTA Code\":   \"NTA2020\",\n",
    "    \"Population\": \"population\",\n",
    "    \"NTA Name\":   \"NTAName_pop\"\n",
    "})\n",
    "\n",
    "# 5) Merge on NTA2020 code\n",
    "nta_polys_gdf = nta_polys_gdf.merge(\n",
    "    nta_pop_df[[\"NTA2020\", \"population\"]],\n",
    "    on=\"NTA2020\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# 6) Exact‐name fallback\n",
    "nta_pop_df[\"NTAName_lc\"]    = nta_pop_df[\"NTAName_pop\"].str.lower()\n",
    "nta_polys_gdf[\"NTAName_lc\"] = nta_polys_gdf[\"NTAName\"].str.lower()\n",
    "pop_lookup = nta_pop_df.set_index(\"NTAName_lc\")[\"population\"].to_dict()\n",
    "mask_missing = nta_polys_gdf[\"population\"].isna()\n",
    "nta_polys_gdf.loc[mask_missing, \"population\"] = (\n",
    "    nta_polys_gdf.loc[mask_missing, \"NTAName_lc\"].map(pop_lookup)\n",
    ")\n",
    "nta_polys_gdf = nta_polys_gdf.drop(columns=\"NTAName_lc\")\n",
    "\n",
    "# 7) Record initial unmatched set\n",
    "initial_unmatched = nta_polys_gdf[nta_polys_gdf[\"population\"].isna()].copy()\n",
    "initial_count     = len(initial_unmatched)\n",
    "\n",
    "# 8) Helpers for fuzzy matching\n",
    "def normalize_string(s: str) -> str:\n",
    "    return re.sub(r'[^A-Za-z0-9 ]+', ' ', s).strip().lower()\n",
    "\n",
    "def common_prefix_len(a: str, b: str) -> int:\n",
    "    for i in range(min(len(a), len(b))):\n",
    "        if a[i] != b[i]:\n",
    "            return i\n",
    "    return min(len(a), len(b))\n",
    "\n",
    "def substring_match_len(a: str, b: str) -> int:\n",
    "    matcher = SequenceMatcher(None, a, b)\n",
    "    match   = matcher.find_longest_match(0, len(a), 0, len(b))\n",
    "    return match.size\n",
    "\n",
    "pop_names   = nta_pop_df[\"NTAName_pop\"].tolist()\n",
    "name_to_pop = nta_pop_df.set_index(\"NTAName_pop\")[\"population\"].to_dict()\n",
    "\n",
    "# 9) Compute best matches & confidences\n",
    "records = []\n",
    "for raw in initial_unmatched[\"NTAName\"]:\n",
    "    norm_raw = normalize_string(raw)\n",
    "    best_pref, best_sub, best_glob = None, None, None\n",
    "    score_pref, score_sub, score_glob = -1.0, -1.0, -1.0\n",
    "    for cand in pop_names:\n",
    "        norm_cand = normalize_string(cand)\n",
    "        p = common_prefix_len(norm_raw, norm_cand) / (len(norm_cand) or 1)\n",
    "        if p > score_pref:\n",
    "            score_pref, best_pref = p, cand\n",
    "        s = substring_match_len(norm_raw, norm_cand) / (len(norm_cand) or 1)\n",
    "        if s > score_sub:\n",
    "            score_sub, best_sub = s, cand\n",
    "        g = SequenceMatcher(None, norm_raw, norm_cand).ratio()\n",
    "        if g > score_glob:\n",
    "            score_glob, best_glob = g, cand\n",
    "    records.append({\n",
    "        \"Unmatched NTA Name\":   raw,\n",
    "        \"Prefix Match\":         best_pref,\n",
    "        \"Prefix Confidence\":    score_pref,\n",
    "        \"Prefix Population\":    name_to_pop[best_pref],\n",
    "        \"Substring Match\":      best_sub,\n",
    "        \"Substring Confidence\": score_sub,\n",
    "        \"Substring Population\": name_to_pop[best_sub],\n",
    "        \"Overall Match\":        best_glob,\n",
    "        \"Overall Confidence\":   score_glob,\n",
    "        \"Overall Population\":   name_to_pop[best_glob],\n",
    "    })\n",
    "\n",
    "matches_df = pd.DataFrame(records)\n",
    "\n",
    "# 10) Prefill any 100% matches\n",
    "filled_by_prefix    = set()\n",
    "filled_by_substring = set()\n",
    "filled_by_overall   = set()\n",
    "for _, r in matches_df.iterrows():\n",
    "    nm = r[\"Unmatched NTA Name\"]\n",
    "    if r[\"Prefix Confidence\"] == 1.0:\n",
    "        nta_polys_gdf.loc[nta_polys_gdf[\"NTAName\"]==nm, \"population\"] = r[\"Prefix Population\"]\n",
    "        filled_by_prefix.add(nm)\n",
    "    elif r[\"Substring Confidence\"] == 1.0:\n",
    "        nta_polys_gdf.loc[nta_polys_gdf[\"NTAName\"]==nm, \"population\"] = r[\"Substring Population\"]\n",
    "        filled_by_substring.add(nm)\n",
    "    elif r[\"Overall Confidence\"] == 1.0:\n",
    "        nta_polys_gdf.loc[nta_polys_gdf[\"NTAName\"]==nm, \"population\"] = r[\"Overall Population\"]\n",
    "        filled_by_overall.add(nm)\n",
    "\n",
    "# 11) Prefill by global≥0.80 & (prefix or substring)≥0.45 same candidate\n",
    "filled_by_fallback = set()\n",
    "for _, r in matches_df.iterrows():\n",
    "    nm = r[\"Unmatched NTA Name\"]\n",
    "    if pd.isna(nta_polys_gdf.loc[nta_polys_gdf[\"NTAName\"]==nm, \"population\"]).any():\n",
    "        cond = (\n",
    "            r[\"Overall Confidence\"] >= 0.80\n",
    "            and (\n",
    "                (r[\"Overall Match\"]==r[\"Prefix Match\"]    and r[\"Prefix Confidence\"]>=0.45)\n",
    "             or (r[\"Overall Match\"]==r[\"Substring Match\"] and r[\"Substring Confidence\"]>=0.45)\n",
    "            )\n",
    "        )\n",
    "        if cond:\n",
    "            nta_polys_gdf.loc[nta_polys_gdf[\"NTAName\"]==nm, \"population\"] = r[\"Overall Population\"]\n",
    "            filled_by_fallback.add(nm)\n",
    "\n",
    "# 12) Final still‐missing\n",
    "still_unmatched = nta_polys_gdf[nta_polys_gdf[\"population\"].isna()]\n",
    "final_count     = len(still_unmatched)\n",
    "matched_count   = initial_count - final_count\n",
    "\n",
    "# 13) Display table if needed\n",
    "if final_count:\n",
    "    disp = matches_df.loc[\n",
    "        matches_df[\"Unmatched NTA Name\"].isin(still_unmatched[\"NTAName\"])\n",
    "    ].copy()\n",
    "    # strip special chars in display\n",
    "    for c in [\"Unmatched NTA Name\",\"Prefix Match\",\"Substring Match\",\"Overall Match\"]:\n",
    "        disp[c] = disp[c].str.replace(r'[^A-Za-z0-9 ]+', ' ', regex=True).str.strip()\n",
    "    display(Markdown(\n",
    "        \"## NTAs Still Missing Population\\n\\n\"\n",
    "        + disp[[\n",
    "            \"Unmatched NTA Name\",\n",
    "            \"Prefix Match\",\"Prefix Confidence\",\"Prefix Population\",\n",
    "            \"Substring Match\",\"Substring Confidence\",\"Substring Population\",\n",
    "            \"Overall Match\",\"Overall Confidence\",\"Overall Population\"\n",
    "          ]].to_markdown(index=False)\n",
    "    ))\n",
    "\n",
    "# 14) Print summary\n",
    "print(f\"Total NTAs processed:             {len(nta_polys_gdf)}\")\n",
    "print(f\"NTAs initially missing population: {initial_count}\")\n",
    "print(f\"  • Filled by 100% prefix match:   {len(filled_by_prefix)}\")\n",
    "print(f\"  • Filled by 100% substring match:{len(filled_by_substring)}\")\n",
    "print(f\"  • Filled by 100% overall match:  {len(filled_by_overall)}\")\n",
    "print(f\"  • Filled by fallback (80%+ & 45%+): {len(filled_by_fallback)}\")\n",
    "print(f\"NTAs still missing population:     {final_count}\")\n",
    "if final_count == 0:\n",
    "    print(\"✅ All NTAs now have a population value.\")\n"
   ]
  },
  {
   "attachments": {
    "image-2.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAABUYAAABKCAIAAAD42RnqAAAgAElEQVR4Ae1dvZLjPA68J/GbzDvMG2x20Tnf6AJnV186obOpumQjZxs5msjJJH6gq9uu6sIHSDShsceg1BtsURQIAt3gDyjJ84/dA/5dr9cHaJVKIVARAUV7RVZk0wwCCtcZYIasFpulaBMdpeiQMSkEFL0puIYWFtcD0Zci6x+PcCxlwSMMkE4h8G0IKNq/DWp19HUEFK5fx7COBrFZh4vdbic6StEhY1IIKHpTcA0tLK4Hoi9FllL6gZiVqRURSI23ig7Ipi0hoHBdE9tisxSboqMUHTImhYCiNwXX0MLieiD6UmQppR+IWZlaEYHUeKvogGzaEgIK1zWxLTZLsSk6StEhY1IIKHpTcA0tLK4Hoi9FllL6gZiVqRURSI23ig7Ipi0hoHBdE9tisxSboqMUHTImhYCiNwXX0MLieiD6UmQppR+IWZlaEYHUeKvogGzaEgIK1zWxLTZLsSk6StEhY1IIKHpTcA0tLK4Hoi9FllL6gZiVqRURSI23ig7Ipi0hoHBdE9tisxSboqMUHTImhYCiNwXX0MLieiD6UmQppR+IWZlaEYHUeKvogGzaEgIK1zWxLTZLsSk6StEhY1IIKHpTcA0tLK4Hoi9FllL6gZiVqRURSI23ig7Ipi0hoHBdE9tisxSboqMUHTImhYCiNwXX0MLieiD6UmQ9KqW/6p8QEAJCQAgIASEgBISAEBACQkAICAEhkEeg/wDiUSl9vwWSFAJDI5A6QhvaUxm/AgQUrisgkS6ITUJRoSA6KrAgG5YhoOhdhtuIrcT1QKylyFJKPxCzMrUiAqnxVtEB2bQlBBSua2JbbJZiU3SUokPGpBBQ9KbgGlpYXA9EX4ospfQDMStTKyKQGm8VHZBNW0JA4bomtsVmKTZFRyk6ZEwKAUVvCq6hhcX1QPSlyFJKPxCzMrUiAqnxVtEB2bQlBBSua2JbbJZiU3SUokPGpBBQ9KbgGlpYXA9EX4ospfQDMStTKyKQGm8VHZBNW0JA4bomtsVmKTZFRyk6ZEwKAUVvCq6hhcX1QPSlyFJKPxCzMrUiAqnxVtEB2bQlBBSua2JbbJZiU3SUokPGpBBQ9KbgGlpYXA9EX4ospfQDMStTKyKQGm8VHZBNW0JA4bomtsVmKTZFRyk6ZEwKAUVvCq6hhcX1QPSlyFJKPxCzMrUiAqnxVtEB2bQlBBSua2JbbJZiU3SUokPGpBBQ9KbgGlpYXA9EX4ospfQDMStTKyKQGm8VHZBNW0JA4bomtsVmKTZFRyk6ZEwKAUVvCq6hhcX1QPSlyFJKPxCzMrUiAqnxVtEB2bQlBBSua2JbbJZiU3SUokPGpBBQ9KbgGlpYXA9EX4ospfQDMStTKyKQGm8VHZBNW0JA4bomtsVmKTZFRyk6ZEwKAUVvCq6hhcX1QPSlyFJKPxCzMrUiAqnxVtEB2bQlBBSua2JbbJZi8150nP78q+DaN1jy8vJyPp9Pp1MFf7dsw72id8sYjuK7uB6Fqd1ulyJLKf1AzMrUigikxltFB2TTlhBQuK6JbbFZis0sHcfj8Wr+Ma39hkS6E7dvsKSd0juIPj8/9/t9p/ELxA6HgyHk/0WSskCbbfL6+nq5XI7Ho60sVc5GbynjZUwKAXGdguu5wimylNI/lyz1PjwCqfE2vLdyYHAEFK6DE/g388Xm3+B49kU/Hchjz+fzy8sLrN7v979+/UL5GxLpTqi+wZKbKf3lcnl9fSUyD8rqkXLbvna73X6///37NznqBG1SzKX0+/3+crk89Hhi0oxGZX/0NpTo1hAIiOshaIKRKbKU0g/ErEytiEBqvFV0QDZtCQGF65rYFpul2Oyn43A4NFLTb0ikO3H7BktSKb1LjDu9uCnWtuFm8wUCSukXgKYm90Kgf6a6V4/SsxiBFFlK6RfjrIZC4P8IpMabIBMCz0VA4fpc/O/bu9i8L55f1NZPx/F4dA+EbdffkEjb7hrlb7CknU47oB6U0rteGoDc65ZS+nshKT0LEOifqRYoV5P7IpAiSyn9fcGXts0hkBpvm0NHDhdDQOFajJAvmSM2vwTfvRv307Hf7z8/P+e+rEYiDRl82u0k7Rfm7pZtZV/sPx6P5/P5x48fl8sFOt2ZgtXJhostQe7N79LdF+m2r/f398bP47lkG94dDgdQN9nL5BmBa2iZh7yD0QqwbLuz6CE///nz5/l8vl6vx+Mx1vAwAt0RGcjvdjv7Gb9V3u50v9+fTqdJQml2Z6E/ejsVSqwsAuK6LDXRsBRZSukjgKoRAgkEUuMtoVeiQuABCChcHwDq01SKzadBP9Vxig6ktTZ5o0okacyrkewxjz2dTmyFTJXpqJVE6kglyI0/Pj7wXTruUo/NnF9eXt7f3/EB+WJLjscjDXZGWvuZ37qcn1A4w1zyP9eLbQVVp9OJUFA5Co1s30o2vEAC//HxwW/jYw1TenZqv6V3H2K8v7+Dpnann5+f1+sVODu6reWd5VT0duqUWE0ExHVNXiatSpGllH4SQ1UKgV4EUuOtV6nkhMBjEFC4PgbX52gVm8/BfabXLB1I2K7XK1NrKLZJ7263Q7aGpDfmn0xfrRj0WOHj8ei+3rdJ5twL9sssifBQP3Jdpr673Q5mNFJ6+0DbAeU6Yi/WNXbBgw/XyqJkcXOPvt2hgO0CGqz+WGPl8cN7NqXHOxTxd/hSnbpzAefmzcts9N5UKIGyCIjrstREw1JkKaWPAKpGCCQQSI23hF6JCoEHIKBwfQCoT1MpNp8G/VTHy+hA+mfzbaam7ISpXcz9mMjNZctINWNDewSAVwZsUoqul1mCttDPhBwPydtm0F8WeGCBGryGYO2c7GW32xExvNOOswAnjMqY0rP3w+EAGSTkfO/g5jlL1NlO6eGXO9e42ak9FMAxwefnpzWSjvQUlkVvj2bJVENAXFdjpGFPiiyl9A0kdUsI3EYgNd5uq5OEEHgkAgrXR6L73brF5ncj3uxvMR1INflmeCOR5ofTTJWv1yuOA5AW2nqUe1J6+y03zUBi7PJMpsoNS9CQX4nbBJvNCSR8d73wrkvpd7udrYENzPCtcp50THpB/e1n+EzpkaJHbGF2PEyJNe2Ungm5fWVjQadK6S2zKs8hsHimmlOo+schkCJLKf3jiJDmTSCQGm+bQEROFkZA4VqYnLRpYjMN2SMbfIWOdgrKZDU+5aZDMY3kLWTCNlfnc2bmwxBG5sn32xuHCw1LrC9Q27A/m9IzzW70Yr2Lz7otLM48d4t9tZVE5GPNzZQeXQMNnNEs6FQpvWNQl5MIfGWmmlSoyschkCJLKf3jiJDmTSCQGm+bQEROFkZA4VqYnLRpYjMN2SMbfIUOZo+TT5WZEsc8lg65pJH1KMRv6ePL4ZC06WgjpW9Y4m7BMBwouFt8Or3gKb1TZXuhy+fzGb+oHz9Tt/gACne6Ed/YnzPSIhYxRI1jJzahPZA8HA7tw46oYY5Qam4XvhK9bc26Ww0BcV2NkYY9KbLGTunjpNbApeatp7jQOF+viVJlq1Lj7SuOrIC1FbjwFQYrtP22cK3g7OptEJulKO6n4/jnH413KWUjkUaax6foSInf3t6gCp/E82tq+9v1uMUH9S4B5k+sxzfbXR7Lw4WGJdGd6/WKrl0rXF6vV9cLkbGv2fPrAOTejV7QHJury+USc3XqZ2Hya3bbe/yMn6DFXVyscSm9u3x7e+NPBtqjimynekpPQlVoINA/UzWU6Nb3IJAi68kpPSd0+5ESV52beMV582aTHoH4TRrXyJ7mKZmUC1iVLVbLDFNmleKoLdw/3rCKW/oaW5nY6YNYs7sW2+lcvZXhM5bOOHyQC84kXTYQ6A/XhpJN3ULmgGE7l3hYGfsJMT8qdqOef3oqzgk2W7uJ83bYBFBub2A/555L22JSRFT3+/3v37/d81tL5Rzd1OAK/XS0l/JGSo8ereMuYNzuhTMz5t6//vqLoWi9m1O42BJrxuFw4EEAX4mHGZ+fn/hz7tYYi6oDyv6IIDN8qHK9ECjXxCp35cntKAHkYkcAGXJxFxdrXA6PoxOowvkO1TqDbUDauSV2AUlrsHOwfdkfvW09ugsE7NzuJi5C5ELOitkhydjQwkHovl6wCHMsW7WYfOyAsk1Air2Lti8vL79//+YJHSptMMzNdbZrW04NzBIpvfMQqE1CbP3EDOt+89MJLLu0p6SceSNzy5S7VnFedgL20mVZjc2KbRXLyqwiJotr+sdbXNQ5ofdE14NYc0FFHObqKYBCahvRdiE1FpwZuuxEoD9cOxWuW8yGN8avW63gvnvIZnfeER87CqCzZ/hHPbvdbjtsYnfltrzkYm4p5AQbtxNRIVNE0IG27GISf1dZmQ4bdc7sFV/GI4kVO/tF1ypH7xdd+/7mdrOHmcTOXbRnv9/zFRs0mRSDvB3CWjiI4YJCgxFqA8I8Q0H9zfkEGwZ3KgdVWEp6uqYNKKQGZsWU3q2szj17+aAcwKX0C5Z2a2S7nHIhZln28Lvdkb1rpwZbr/ICBPrHGwb25ObSTQGTZjyItRhU6H2uftK2zsq2C6mx0NmjxBwC/eHqGm7z0i3hbmmYw6QxLdszAv7atlL6OSRRz92V3e860CYx57NiO+uykm+Doxcs9FYyOyNVHlztubeN/6B33Vgb1ItvM7ty9H4bCPfqyA23zlBsrC9OA6ZENwf2G79xricXC4ce3w+yILv9gG3CRYp/goR33V7aUUmxuUKKrKIpfWcWnV1x5yBz9W5cdRrjlHReplxwkYGzD/feXU+/brrpaSKZOQT6x9tcSj9X73p8EGsxqNDvXL2zKnXZdiE1FlL9SpgI9Icrm2y2ELdNXx+qbgh8MeY3wubxeMReyqb0LizjfIWF++3tzX1NTQpYgKq404pJvuvUXVamwznrLF/lZc/GfZWOL3OqcvQu8+hZreK80ZlENJYDN34bkj1eb5nruKxHxF5fXz8+Pt7e3uyPU0RabUMywgLvuomoxwC2zb6LVzSl52+0/POf/zyfz+7lNy69DjucvnPVB3D44MHmvRgbP3/+/Pzzz33zgDzZPjWN27g5zXxLH51aJe4TMn5ETRf4kYY11fJKTF5fX1nPn8lFtPGTG4KAVufz2frrZof+Lx3YrwpEoH9yjIFEJRjz//73v+0MgrvcqrZZw6BAALTZZ6dO/816hqh9GYmDkc15unm9XvFrwxy/cOHHjx+XywWm2peRGL18b9kONNsp+1Ihi0B/uGY1r0/ene1yDmc8z7k8d5YfZwDO/3Oq2vVbYJMQuY2RQ4bzJOvtbGOfvTsBfksfp7LOjTgVVqbDLR+0eZUFLFV2HVylm/d1qnL03tfTR2uLMwl+VOVmQHI/7yzUwuEA+crlHMhWJ1Zwx2M7pWdzLliscWtTFKDkZCE1MOum9NxOOTjs2LDQuK/pQAYX8tPpxFQZCuPv4hBNdo0at5NoaH59fT2dTtwi2E4xJjmkX19f39/f7Q+u4O0OBA3FaBIKDSgOhwOdRV/cd0Z/7ep+Op3c0YPrVJdtBPrHW5yXqRkcxQMs26TBGoJ/MoQi++wUhRhUsd6Fpe3OzXo25tGKp1c88OJIdEPJDme+k8yoPvz554zXZRaB/nDNal6f/OTaP5eu0/25ATV3JmtPsjhpU1u7sHo27S7KLcQWGYhZ9DiZ2CnUNuFhN9frKIkaq9ZpcJerp8P5q8s1IaDovRebnHysQrt/s/Usu60U67VwWCi+XgYReMZpHyxRM9d9xwiWA67X3MeyIQqRfbuOxKXKNY+XqYE5QErvFlp7SexcPm/TfgBkW8U/0OpAhDYy5zJet7Gwmp0emhftoaTLauI7ApSMA7vhiJ0+ohjvKp+38C4r94+3RqgwDXDHSfZyjrU4R9iZKLLv3LQP1RnzKHDOsmagOUeB7csGPMTgMvfE0Rjqib92GbU5y3W5AIH+cF2gfGVNuLRbv2zEunqMGka7vcsn/Dyicnd5vDt3nhvls6/kTWooXslJr7GG4hYnKwd1e9Z1aGMy5MeTv379ulwuc4RG6DS4IiaqGQUBRe+9mJrcunCP53phumFnMCtjjzVtPcvYg7mpjHcnC1vm2r3CA/Q4yWO9wBJgN7cORpDickPITLIPte0dguuClymyBkjp3Vput1nADh882K2SZQW42LRnbmgRQZfAWMrbmqGBQxT8HQ4HtLIWsq9IfyOMXPYVpwD7ajTvRn+xT3p/f5+MSNqmQg8C/eOtEQbkyMnYR4JzrMUQskqoec6XOQFbb82AHt614cpK9mXHXXws5k6pnCPwgmFMnSp8BYH+cP1KL+toa9caehTHAm+hgPmfWwTedeHNeluwo8nWz5XXzaZbiCcPUzBLuIXMStrJ0MFozwt4yy6y+LtokUoKu8K66XDO6nJlCCh670Xo5FQ/OdvYHuc2PJPabEMeB/Ms0t2Nl1vmOj7I5KMmt19tL8dOmCBHvtxCxu7YpF1IkVU3pbcDwCJid1RA3P1uLeMbGbX9H2uz1TwJpe0OAqxhj1YtXy3GXSYhDAgWYneR/oZwTJmoEBsRbj6sZPSXu5bJUwbqVKEHgf7x1thc2qjmfhTynKbnWHNHSIxMkBvZd07ZULG3WI+Zi2pZQJzbcKXl1ONmvWgMe4lP6fm0DT0SBypXYQEC/eG6QPnKmnDap18unlnvCrGhO5h28rxsTBGUsYUVsxmhiNMLJh/3eMqBH/UQwDgd8RYKoLt/lVwxHQ4ZXa4PAUXvvTi1myLqjNMXb7EwOVktbki1sbBlriOe5MtuR5lINnaeUVXcx0ZOO3cRZC1FVtGU3i2lvAQ6hBj5MJ6l2HXdiREaFG4u5G5PYElqaI48MVAare6S0rMjempDM/qLmr/++ou/Q8aGKmQR6B9vCIO4QXThwZBwDwnnWKP8pOWRfSdmQ8XesvX2xMHKuFkvTnBuUERjbC8NR3CcEaFzxujyJgL94XpT1eoF4rw6N4QdFLEhlzAn6S479bPVitnkCSbPEFnAPDD5NgRwpqQtuCf5ky8NEVgUGjOSk8TliumY9FeVa0JA0XsvNuM07jZCcx1FMS0cc1gtrrd7TijBPP+f//yHP9tsF47r9crHtK7TuOO12SKVT/7otc1YnVp3mRqYRVN6+yNbcA/JAH5Am79nwxUXQ4gYxYFhMYp5hb07+TU7k/yG5ngLm5LD4RBvsUe6YGtiBOBujEXUux0kumMURn9Zg12RMiWCv6DQP97iRM8H0Qxd1iDULTVzrE2qpSNsxRpXmAsqW2/LrrmNPQ4TyuCufXnEetp+8Z5KUGgcKzhJXTYQ6A/XhpKN3IrzdozwSSiiWJznOxtOirFyU2za/VOkhpi4QmN6vDk32h6d2snLmnQAAT4ImbT8WZU3KXiWYT39Vga2x34nUzN6nZGjXLqpw26TGi7EyUoLRwOuZbciF3P72yhpe4xk4a6jLIr1L15QmBqY5VJ6+M90lAgCJvcHZi12QJ8JQ0xW39/f8effbq4ibkMGzUxLGprtSQRa8S9vOSX2F+8vl4v9Q3qQnFyA5yIPoNFCHCUQw+ivrdHzT8bYskL/eItjG1wzaGkAiHaB0WCNh0fQ8PLy8v7+jpMv24r6bWEuqGw9LLd28vfnbbi6syRc8rOUycdisReeYuz3+7e3N5gaobMuqNyPQH+49utcsSRme8zGLgj5RZz7QycYEQxjgGPj3ML19vbGyR8NOY1bsbnypti0u2S79M+Bg3rHmhWOc6Olg/zaJu1yPx2cG/k4yM6u7V6yd4HA5I5iThXNm2sFndxjzOm5WR8puNnkEQL0N0XHAmAfYfy9dPZH7716XLEeuwogujix24nl169fnP8h5uYBLRyPCBKLqmXK9YVbnAPtjhRkTU6AcW2yjGMbzMTQ9Th5mRqYJVJ6TqMoEEHnXvxVA4cdCOD7dbikcu6xbq4i2MaxYUR/TrNdGPin4OkOFgCqRb1zwb3J7BCwgehuWZOOf/4x2qK/rgYJoZtKnH5dziHQP95cAFyvV8aqUw5Jx0ibNRe0jDrXynXknpPbuy7YbGzbX69A4LE7K/b5+fnz58/z+WwXs+gUA5WTHb4HsSGtL0QsNV8p94frV3pZU1s7sriIIFY5fu3vksb1ovEhPeZetyj0o7cpNl1K//n5SdxYsAQBRsylsR4MxumIqtytHlL66cA8yYlxcsLv6bFHBso5Rfc04TQ+BwLi1k7dPWqjzM3lKTZBzeTGyR2Cz7WN9cvoWABs7LpOTX/01rG5siV2A8OR7hYOu7jYhx/0y056rLQ7JcxXqdG9hT+VYrGaLNuVdw49t7m1hE6ShY7i1IRXv7myZKfN1MB8cko/ifVcpd68nUNG9U9EIDXeOu1czV4BW6XJ/XQnFBK7LwKPCNf7Wiht/QiIzX6svkGynw6XQ05+7ncvgxesJjDv8ucfnyLSHtz9+PjI7k2pgYWaKX0nHQuApeMFC/3RW9B4mZRCQFyn4HqucIqsYVJ6d17yXIjVuxAgAqnxxlbtgntC3haufFfDtho7jwjXaj5uxx6xWYrrfjqGSOl///5t37Ei1Pjd1l+/fimlv1wuc4/4CNcohf7oHcUj2TmHgLieQ6ZgfYqsYVL6ufdPChIgkzaFQGq89SCDs/8Rn2y7j4rhiH3lrMd9yTwUgbuH60OtlfI2AmKzjc833+2nI6b08RjXvhpqlwP3sq5LKe1dfBjCh8lUyA9G5vChedGquVtU7j4ow3P4Hz9+8Nek7YrQvou++MIqvgJwldfr9b///e/5fKYYv9LCe532dVkLo/WdTrEyOn4XYKm/YKE/egsaL5NSCIjrFFzPFU6RNUBKj88U577pei7W6l0IpMZbGy7svbgjaQsXvBs3W3NbqILGb8SkO4brRhCr7KbYLMVOPx0uh4xvM8Wf2sVc+vLycjqd8EO/7rtcfmHLJB+/9heXFat8EkCah7Z2GueXojbpdYe5Vj9SfT7Ph6dU2L57OBwoCUt4HEAzaH+scVtH5OQEhw35V2ascvdXh2An2y4G1nZardwfvdUslz1ZBMR1FrEnyqfIGiClfyKU6loI3EQgNd5uapOAEHgoAgrXh8L7zcrF5jcD3u6un454+sl8cvIncm3+bG1AootUM54LQNIlw5P6rU6X4rqu+b6kq7cabHbtfu3Z/VRk+67ViQMLPtqxXUAs1tiTBeeU05ylA80XAOv6LXXZH72lzJYxCxAQ1wtAe1aTFFlK6Z9Fk/pdCQKp8bYSn+XGsAgoXIelbsJwsTkByvOq+ungY3Aaezqd+D58/NG4yT+sy1fN8Sh7Lse2aT+6Qw1a2ffJ+YKYNc+eFFhVsTunCgcN0RfbsH0X1to/J2Gf9rvft59M6e1BiTtNIPKT2b6jg/3aVhaNCKyVHKLcH71DuCMjGwiI6wY41W6lyFJKX40+2TMYAqnxNphvMnd1CChc10Sp2CzFZj8dNmeGC7bGJrHM25HwI41k8m+Tcz4/d5gsyDytMTYTthm4zcyR9jPvtacAtgkMsw1v3rV/LMo2jAl8rIl/I2kOIuevS/LnWi0A1lFT6rI/ekuZLWMWICCuF4D2rCYpspTSP4sm9bsSBFLjbSU+y41hEVC4DkvdhOFicwKU51X10xFzSGTOeKocE1365NJLm9LPtVqQeTrz8I4AfuieH7czwXbC7sX+aBUbunfp4SPv2nMBdwtdPPQpfQ8dC4AljwUL/dFb0HiZlEJAXKfgeq5wiiyl9M8lS70Pj0BqvA3vrRwYHAGF6+AE/s18sfk3OJ590U9HTINtjXvN3rrlUnq8685X6Pn03jZZkHlaY/jI2v0heubeTtj9St+9Unr0Yl8EeGhKb52ao2MBsJaXauX+6K1muezJIiCus4g9UT5FllL6JzKlrteAQGq8rcFh+TAyAgrXkdnztotNj8hTr/vpsBkjTLYfb7v0FQ+l397ekC0zb0dKyQ/g8WCZd3e7nf1hdv5a+263Q0M+b4+YRfNwdmA/TWdKj35tsv35+Xm9Xr/4LT2MZI/ut/GjC7HmKy/eWzruCGyEuk5Nf/TWsVmWLENAXC/D7SmtUmQppX8KR+p0PQikxtt63JYnYyKgcB2Tt2mrxeY0Lk+q7acDOTO/k79er/wtd9puv6hnwoz0Eg0vl8u//vWvy+Vik3OkvhCATuS6X0zpX19fPz4+9vs9zbMpvXXnfD7//PmTfwRu8VN6vsAPX45//lkc6CnddzXZlL5NB5WTrAXAEr2Chf7oLWi8TEohIK5TcD1XOEWWUvrnkqXeh0cgNd6G91YODI6AwnVwAv9mvtj8GxzPvhAdz2ZA/S9HQNG7HLvRWorrgRhLkaWUfiBmZWpFBFLjraIDsmlLCChc18S22CzFpugoRYeMSSGg6E3BNbSwuB6IvhRZSukHYlamVkQgNd4qOiCbtoSAwnVNbIvNUmyKjlJ0yJgUAoreFFxDC4vrgehLkaWUfiBmZWpFBFLjraIDsmlLCChc18S22CzFpugoRYeMSSGg6E3BNbSwuB6IvhRZSukHYlamVkQgNd4qOiCbtoSAwnVNbIvNUmyKjlJ0yJgUAoreFFxDC4vrgehLkaWUfiBmZWpFBFLjraIDsmlLCChc18S22CzFpugoRYeMSSGg6E3BNbSwuB6IvhRZSukHYlamVkQgNd4qOiCbtoSAwnVNbIvNUmyKjlJ0yJgUAoreFFxDC4vrgehLkaWUfiBmZWpFBFLjraIDsmlLCChc18S22CzFpugoRYeMSSGg6E3BNbSwuB6IvhRZSukHYlamVkQgNd4qOiCbtoSAwnVNbIvNUmyKjlJ0yJgUAoreFFxDC4vrgehLkaWUfiBmZWpFBFLjraIDsmlLCChc18S22CzFpugoRYeMSSGg6E3BNbSwuB6IvhRZSukHYlamVkQgNd4qOiCbtoSAwnVNbIvNUmyKjlJ0yJgUAoreFFxDC4vrgehLkfWolP6qf0JACN65FqwAAADfSURBVAgBISAEhIAQEAJCQAgIASEgBIRAHoH+A4hHpfT9FkhSCAyNQOoIbWhPZfwKEFC4roBEuiA2CUWFguiowIJsWIaAoncZbiO2EtcDsZYiSyn9QMzK1IoIpMZbRQdk05YQULiuiW2xWYpN0VGKDhmTQkDRm4JraGFxPRB9KbKU0g/ErEytiEBqvFV0QDZtCQGF65rYFpul2BQdpeiQMSkEFL0puIYWFtcD0ZciSyn9QMzK1IoIpMZbRQdk05YQULiuiW2xWYpN0VGKDhmTQkDRm4JraGFxPRB9KbL+BwfwxE2EYsbPAAAAAElFTkSuQmCC"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROBLEM MED OVENSTÅENDE STRING MATCH\n",
    "\n",
    "![image-2.png](attachment:image-2.png)\n",
    "\n",
    "- Nogle har ikke et godt match.\n",
    "\n",
    "- Nogle får også et match grundet \"BlaBlaBlaBla-South\" får en høj prefix confidence på \"BlaBlaBlaBla-North\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amenities data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amenities_data_path = \"./data/NYC_Amenities.csv\"\n",
    "\n",
    "# Function to extract coordinates from Point objects or calculate centroid for Polygons\n",
    "def extract_coordinates(geometry):\n",
    "    if isinstance(geometry, Point):\n",
    "        return geometry.x, geometry.y\n",
    "    elif isinstance(geometry, Polygon):\n",
    "        centroid = geometry.centroid\n",
    "        return centroid.x, centroid.y\n",
    "    return None, None\n",
    "\n",
    "amenities_df = None\n",
    "amenities_gdf = None\n",
    "\n",
    "# Apply the function to the dataset\n",
    "if os.path.exists(amenities_data_path):\n",
    "    print(\"Loading amenities data from local file...\")\n",
    "    amenities_df = pd.read_csv(amenities_data_path, low_memory=False)\n",
    "    # convert to geodataframe\n",
    "    amenities_gdf = gpd.GeoDataFrame(\n",
    "        amenities_df,\n",
    "        geometry=amenities_df[\"geometry\"].apply(loads),\n",
    "        crs=\"EPSG:4326\"\n",
    "    ).to_crs(epsg=3857)\n",
    "else:\n",
    "    print(\"Querying OSM for amenities data...\")\n",
    "    # Define a dictionary of tags for the amenities you're interested in\n",
    "    tags = {\n",
    "        \"amenity\": [\"bar\", \"restaurant\"],\n",
    "        \"leisure\": \"park\",\n",
    "        \"railway\": \"station\"\n",
    "    }\n",
    "\n",
    "    # Use OSMnx to query OSM for these features in New York City\n",
    "    amenities_gdf = ox.features.features_from_place(\"New York City, USA\", tags)\n",
    "\n",
    "    # Extract coordinates or calculate centroid\n",
    "    amenities_gdf[['Longitude', 'Latitude']] = amenities_gdf['geometry'].apply(lambda x: pd.Series(extract_coordinates(x)))\n",
    "\n",
    "    # Save the queried data to a CSV file for future use\n",
    "    amenities_gdf.to_csv(amenities_data_path, index=False)\n",
    "    amenities_df = amenities_gdf\n",
    "\n",
    "# Create a new column 'category' to combine 'leisure' and 'amenity'\n",
    "amenities_gdf['category'] = amenities_gdf['leisure'].combine_first(amenities_gdf['amenity']).combine_first(amenities_gdf['railway'])\n",
    "\n",
    "# 5) Filter to exactly the four types you want\n",
    "keep = [\"bar\", \"restaurant\", \"park\", \"station\"]\n",
    "amenities_gdf = amenities_gdf[amenities_gdf[\"category\"].isin(keep)]\n",
    "\n",
    "for amenity in amenities_gdf['category'].unique():\n",
    "    print(f\"Number of {amenity}: {len(amenities_gdf[amenities_gdf['category'] == amenity])}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VISUALIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "import numpy as np\n",
    "\n",
    "def preview_gdf(\n",
    "    gdf, \n",
    "    name, \n",
    "    groupby=None, \n",
    "    max_groups=None, \n",
    "    cols=None, \n",
    "    n=5, \n",
    "    random_state=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Preview a GeoDataFrame in Markdown:\n",
    "    - Reprojects to WGS84 (lat/lon).\n",
    "    - Computes 'Latitude'/'Longitude' from centroids.\n",
    "    - Always shows the 'geometry' column.\n",
    "    - If `groupby` is provided, samples 1 row per group (up to max_groups).\n",
    "      Otherwise, shows the first n rows.\n",
    "    - You can pass `cols` to include extra columns before geometry/lat/lon.\n",
    "    \"\"\"\n",
    "    # 1) Copy and reproject\n",
    "    df = gdf.copy().to_crs(epsg=4326)\n",
    "    # 2) Compute lat/lon\n",
    "    centroids = df.geometry.centroid\n",
    "    df['Latitude']  = centroids.y\n",
    "    df['Longitude'] = centroids.x\n",
    "\n",
    "    # 3) Decide rows to show\n",
    "    if groupby:\n",
    "        groups = df[groupby].dropna().unique()\n",
    "        if max_groups and len(groups) > max_groups:\n",
    "            rng = np.random.default_rng(random_state)\n",
    "            groups = rng.choice(groups, size=max_groups, replace=False)\n",
    "        df = (\n",
    "            df[df[groupby].isin(groups)]\n",
    "            .groupby(groupby, group_keys=False)\n",
    "            .apply(lambda sub: sub.sample(n=1, random_state=random_state))\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "        title = f\"{name} — Representative (1 per '{groupby}')\"\n",
    "    else:\n",
    "        df = df.head(n)\n",
    "        title = f\"{name} — Sample ({n} rows)\"\n",
    "\n",
    "    # 4) Build column list: user cols + geometry + lat/lon\n",
    "    display_cols = []\n",
    "    if cols:\n",
    "        display_cols += cols\n",
    "    # always include geometry\n",
    "    display_cols.append('geometry')\n",
    "    # then lat/lon\n",
    "    display_cols += ['Latitude', 'Longitude']\n",
    "\n",
    "    # 5) Render Markdown\n",
    "    md = df[display_cols].to_markdown(index=False)\n",
    "    display(Markdown(f\"## {title}\\n\\n{md}\"))\n",
    "\n",
    "def preview_df(\n",
    "    df, \n",
    "    name, \n",
    "    n=5\n",
    "):\n",
    "    \"\"\"\n",
    "    Preview a regular DataFrame in Markdown:\n",
    "    - Schema (column names & dtypes)\n",
    "    - First n rows\n",
    "    \"\"\"\n",
    "    schema = df.dtypes.reset_index()\n",
    "    schema.columns = ['column', 'dtype']\n",
    "    md_schema = schema.to_markdown(index=False)\n",
    "    md_sample = df.head(n).to_markdown(index=False)\n",
    "    display(Markdown(f\"## {name} — Schema\\n\\n{md_schema}\"))\n",
    "    display(Markdown(f\"## {name} — Sample ({n} rows)\\n\\n{md_sample}\"))\n",
    "\n",
    "# === Usage ===\n",
    "\n",
    "# 1) Crime: one per category\n",
    "crime_wgs = crime_gdf.to_crs(epsg=4326)\n",
    "preview_gdf(\n",
    "    crime_wgs,\n",
    "    \"Crime Data (2019)\",\n",
    "    groupby='LAW_CAT_CD',\n",
    "    max_groups=3,\n",
    "    cols=['CMPLNT_FR_DT', 'LAW_CAT_CD', 'BORO_NM']\n",
    ")\n",
    "\n",
    "# 2) Amenities: one per type\n",
    "amen_wgs = amenities_gdf.to_crs(epsg=4326)\n",
    "preview_gdf(\n",
    "    amen_wgs,\n",
    "    \"Amenities Data\",\n",
    "    groupby='category',\n",
    "    max_groups=4,\n",
    "    cols=['category', 'name']\n",
    ")\n",
    "\n",
    "# 3) NTA Polygons (with Population) — only sample from those with a real population\n",
    "nta_wgs = nta_polys_gdf.to_crs(epsg=4326)\n",
    "\n",
    "# filter out any NTAs still missing population\n",
    "nta_with_pop = nta_wgs[nta_wgs['population'].notna()]\n",
    "\n",
    "# pick exactly one representative row (with a real population)\n",
    "preview_gdf(\n",
    "    nta_with_pop,\n",
    "    \"NTA Polygons (with Population)\",\n",
    "    groupby='NTA2020',\n",
    "    max_groups=1,\n",
    "    cols=['NTA2020', 'NTAName', 'BoroName', 'population']\n",
    ")\n",
    "\n",
    "# 4) NTA population table (regular DataFrame)\n",
    "preview_df(\n",
    "    nta_pop_df,\n",
    "    \"NTA Population Data\",\n",
    "    n=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from shapely.ops import unary_union\n",
    "from folium.plugins import HeatMap, MarkerCluster, GroupedLayerControl\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "USE_SAMPLE      = True      # If True, sample a fraction of points for speed\n",
    "SAMPLE_FRACTION = 0.03      # Fraction to sample when USE_SAMPLE=True\n",
    "RANDOM_STATE    = 42\n",
    "\n",
    "# === 1) Prepare GeoDataFrames ===\n",
    "# Assumes crime_gdf, amenities_gdf, nta_polys_gdf are pre-loaded\n",
    "\n",
    "# Reproject all to WGS84 (latitude/longitude) for mapping\n",
    "crime     = crime_gdf.to_crs(epsg=4326).copy()\n",
    "amenities = amenities_gdf.to_crs(epsg=4326).copy()\n",
    "ntas      = nta_polys_gdf.to_crs(epsg=4326).copy()\n",
    "\n",
    "# Also prepare Web Mercator for distance-in-meters computations\n",
    "crime_m     = crime.to_crs(epsg=3857).copy()\n",
    "amenities_m = amenities.to_crs(epsg=3857).copy()\n",
    "ntas_m      = ntas.to_crs(epsg=3857).copy()\n",
    "\n",
    "# === 2) (Optional) Sample for performance ===\n",
    "if USE_SAMPLE:\n",
    "    crime     = crime.sample(frac=SAMPLE_FRACTION, random_state=RANDOM_STATE)\n",
    "    crime_m   = crime_m.loc[crime.index]\n",
    "    amenities = amenities.sample(frac=SAMPLE_FRACTION, random_state=RANDOM_STATE)\n",
    "    amenities_m = amenities_m.loc[amenities.index]\n",
    "\n",
    "# === 3) Extract latitude and longitude for Folium ===\n",
    "crime['latitude']   = crime.geometry.y\n",
    "crime['longitude']  = crime.geometry.x\n",
    "amen_centroids      = amenities.geometry.centroid\n",
    "amenities['latitude']  = amen_centroids.y\n",
    "amenities['longitude'] = amen_centroids.x\n",
    "\n",
    "# === 4) Count crimes and amenities per NTA ===\n",
    "crime_ntas = gpd.sjoin(\n",
    "    crime[['geometry']],\n",
    "    ntas[['NTA2020','population','geometry']],\n",
    "    how='inner', predicate='within'\n",
    ")\n",
    "crime_counts = crime_ntas.groupby('NTA2020').size().rename('crime_count')\n",
    "\n",
    "amen_ntas = gpd.sjoin(\n",
    "    amenities[['geometry']],\n",
    "    ntas[['NTA2020','population','geometry']],\n",
    "    how='inner', predicate='within'\n",
    ")\n",
    "amenity_counts = amen_ntas.groupby('NTA2020').size().rename('amenity_count')\n",
    "\n",
    "ntas = (\n",
    "    ntas.set_index('NTA2020')\n",
    "        .join(crime_counts, how='left')\n",
    "        .join(amenity_counts, how='left')\n",
    "        .fillna({'crime_count': 0, 'amenity_count': 0})\n",
    "        .reset_index()\n",
    ")\n",
    "ntas['crime_rate']             = ntas['crime_count']            / ntas['population']\n",
    "ntas['amenity_per_capita']     = ntas['amenity_count']          / ntas['population']\n",
    "ntas['crime_to_amenity_ratio'] = ntas['crime_count']            / ntas['amenity_count'].replace({0: np.nan})\n",
    "\n",
    "# === 5) Aggregate metrics per borough ===\n",
    "boroughs = (\n",
    "    ntas[['BoroName','population','crime_count','amenity_count','geometry']]\n",
    "    .dissolve(by='BoroName', aggfunc='sum')\n",
    "    .reset_index()\n",
    ")\n",
    "boroughs['density']           = boroughs['population'] / boroughs.geometry.to_crs(epsg=3857).area\n",
    "boroughs['crime_rate_boro']   = boroughs['crime_count']   / boroughs['population']\n",
    "boroughs['amenity_rate_boro'] = boroughs['amenity_count'] / boroughs['population']\n",
    "\n",
    "# === 6) Pre‐compute MultiPoint unions for each amenity type ===\n",
    "unions = {}\n",
    "for amen_type in ['bar','restaurant','park','station']:\n",
    "    pts = amenities_m[amenities_m['category'] == amen_type].geometry\n",
    "    unions[amen_type] = unary_union(pts)\n",
    "\n",
    "# === 7) Compute nearest‐amenity distance for each crime point ===\n",
    "for amen_type, union_geom in unions.items():\n",
    "    # distance in meters to the single closest amenity of that type\n",
    "    crime_m[f'dist_to_{amen_type}'] = crime_m.geometry.distance(union_geom)\n",
    "    crime[f'dist_to_{amen_type}']   = crime_m.loc[crime.index, f'dist_to_{amen_type}']\n",
    "\n",
    "# === 8) Compute average distances per NTA and per borough ===\n",
    "distance_fields = ['dist_to_bar','dist_to_restaurant','dist_to_park','dist_to_station']\n",
    "\n",
    "# Average per NTA\n",
    "c2n = gpd.sjoin(\n",
    "    crime[['geometry'] + distance_fields],\n",
    "    ntas[['NTA2020','geometry']],\n",
    "    how='inner', predicate='within'\n",
    ")\n",
    "dist_ntas = (\n",
    "    c2n.groupby('NTA2020')[distance_fields]\n",
    "       .mean()\n",
    "       .rename(columns={f: f'avg_{f}' for f in distance_fields})\n",
    ")\n",
    "ntas = ntas.set_index('NTA2020').join(dist_ntas).reset_index()\n",
    "\n",
    "# Average per borough\n",
    "c2b = gpd.sjoin(\n",
    "    crime[['geometry'] + distance_fields],\n",
    "    boroughs[['BoroName','geometry']],\n",
    "    how='inner', predicate='within'\n",
    ")\n",
    "dist_boros = (\n",
    "    c2b.groupby('BoroName')[distance_fields]\n",
    "       .mean()\n",
    "       .rename(columns={f: f'avg_{f}' for f in distance_fields})\n",
    ")\n",
    "boroughs = boroughs.set_index('BoroName').join(dist_boros).reset_index()\n",
    "\n",
    "# === 9) Initialize Folium map ===\n",
    "center = [crime['latitude'].mean(), crime['longitude'].mean()]\n",
    "m = folium.Map(location=center, zoom_start=11, tiles=None)\n",
    "folium.TileLayer('CartoDB Positron', name='Basemap', control=True).add_to(m)\n",
    "\n",
    "# === 10) Crime point and cluster layers ===\n",
    "crime_point_layers = []\n",
    "crime_cluster_layers = []\n",
    "for category, color in [('Felony','crimson'),('Misdemeanor','orange'),('Violation','blue')]:\n",
    "    # Individual points\n",
    "    fg_pts = folium.FeatureGroup(name=f\"Crime: {category}\", show=False)\n",
    "    subset = crime[crime['LAW_CAT_CD'].str.title() == category]\n",
    "    for _, r in subset.iterrows():\n",
    "        folium.CircleMarker(\n",
    "            [r['latitude'], r['longitude']],\n",
    "            radius=3, color=color, fill=True, fill_opacity=0.6\n",
    "        ).add_to(fg_pts)\n",
    "    fg_pts.add_to(m)\n",
    "    crime_point_layers.append(fg_pts)\n",
    "\n",
    "    # Cluster markers\n",
    "    fg_cl = MarkerCluster(name=f\"Crime Clusters: {category}\", show=False)\n",
    "    for _, r in subset.iterrows():\n",
    "        folium.Marker([r['latitude'], r['longitude']]).add_to(fg_cl)\n",
    "    fg_cl.add_to(m)\n",
    "    crime_cluster_layers.append(fg_cl)\n",
    "\n",
    "# === 11) Crime heatmap ===\n",
    "crime_heat = folium.FeatureGroup(name=\"Crime Heatmap\", show=False)\n",
    "HeatMap(\n",
    "    list(zip(crime['latitude'], crime['longitude'])),\n",
    "    radius=15, blur=10, min_opacity=0.3\n",
    ").add_to(crime_heat)\n",
    "crime_heat.add_to(m)\n",
    "\n",
    "# === 12) Amenity point and cluster layers ===\n",
    "amenity_point_layers = []\n",
    "amenity_cluster_layers = []\n",
    "for amen, color in [('Bar','purple'),('Restaurant','darkgreen'),('Park','green'),('Station','cadetblue')]:\n",
    "    # Small circle markers\n",
    "    fg_pts = folium.FeatureGroup(name=f\"Amenity: {amen}\", show=False)\n",
    "    mc = MarkerCluster().add_to(fg_pts)\n",
    "    subset = amenities[amenities['category'].str.title() == amen]\n",
    "    for _, r in subset.iterrows():\n",
    "        folium.CircleMarker(\n",
    "            [r['latitude'], r['longitude']],\n",
    "            radius=3, color=color, fill=True, fill_opacity=0.6\n",
    "        ).add_to(mc)\n",
    "    fg_pts.add_to(m)\n",
    "    amenity_point_layers.append(fg_pts)\n",
    "\n",
    "    # Clustered raw markers\n",
    "    fg_cl = MarkerCluster(name=f\"Amenity Clusters: {amen}\", show=False)\n",
    "    for _, r in subset.iterrows():\n",
    "        folium.Marker([r['latitude'], r['longitude']]).add_to(fg_cl)\n",
    "    fg_cl.add_to(m)\n",
    "    amenity_cluster_layers.append(fg_cl)\n",
    "\n",
    "# === 13) Amenity heatmap ===\n",
    "amen_heat = folium.FeatureGroup(name=\"Amenity Heatmap (All)\", show=False)\n",
    "HeatMap(\n",
    "    list(zip(amenities['latitude'], amenities['longitude'])),\n",
    "    radius=15, blur=10, min_opacity=0.3\n",
    ").add_to(amen_heat)\n",
    "amen_heat.add_to(m)\n",
    "\n",
    "# === 14) Choropleth style helper ===\n",
    "def make_choro_style(series):\n",
    "    cuts = series.quantile([0.2,0.4,0.6,0.8]).values\n",
    "    def style_fn(feature):\n",
    "        v = feature['properties'].get(series.name)\n",
    "        if v is None or np.isnan(v):\n",
    "            c = 'grey'\n",
    "        elif v > cuts[3]:\n",
    "            c = 'red'\n",
    "        elif v > cuts[2]:\n",
    "            c = 'orange'\n",
    "        elif v > cuts[1]:\n",
    "            c = 'yellow'\n",
    "        elif v > cuts[0]:\n",
    "            c = 'lightgreen'\n",
    "        else:\n",
    "            c = 'green'\n",
    "        return {'fillColor': c, 'color': 'black', 'weight': 1, 'fillOpacity': 0.6}\n",
    "    return style_fn\n",
    "\n",
    "# === 15) NTA metric choropleths ===\n",
    "nta_layers = []\n",
    "for title, field, aliases in [\n",
    "    (\"Crime Rate\",         'crime_rate',             ['Neighborhood','Crime Rate']),\n",
    "    (\"Amenities/Capita\",   'amenity_per_capita',     ['Neighborhood','Amenities/Capita']),\n",
    "    (\"Crime/Amenity Ratio\",'crime_to_amenity_ratio',['Neighborhood','Crime/Amenity'])\n",
    "]:\n",
    "    fg = folium.FeatureGroup(name=f\"NTA: {title}\", show=False)\n",
    "    folium.GeoJson(\n",
    "        ntas,\n",
    "        style_function=make_choro_style(ntas[field]),\n",
    "        tooltip=folium.GeoJsonTooltip(fields=['NTAName', field], aliases=aliases)\n",
    "    ).add_to(fg)\n",
    "    fg.add_to(m)\n",
    "    nta_layers.append(fg)\n",
    "\n",
    "# === 16) Borough metric choropleths ===\n",
    "boro_layers = []\n",
    "for title, field, aliases in [\n",
    "    (\"Population Density\",'density',           ['Borough','Density']),\n",
    "    (\"Crime Rate\",       'crime_rate_boro',   ['Borough','Crime Rate']),\n",
    "    (\"Amenities Rate\",   'amenity_rate_boro', ['Borough','Amenities Rate'])\n",
    "]:\n",
    "    fg = folium.FeatureGroup(name=f\"Borough: {title}\", show=False)\n",
    "    folium.GeoJson(\n",
    "        boroughs,\n",
    "        style_function=make_choro_style(boroughs[field]),\n",
    "        tooltip=folium.GeoJsonTooltip(fields=['BoroName', field], aliases=aliases)\n",
    "    ).add_to(fg)\n",
    "    # Label boroughs\n",
    "    for _, r in boroughs.iterrows():\n",
    "        folium.map.Marker(\n",
    "            [r.geometry.centroid.y, r.geometry.centroid.x],\n",
    "            icon=folium.DivIcon(html=f\"<div style='font-size:12px'><b>{r['BoroName']}</b></div>\")\n",
    "        ).add_to(fg)\n",
    "    fg.add_to(m)\n",
    "    boro_layers.append(fg)\n",
    "\n",
    "# === 17) NTA average distance to each amenity type ===\n",
    "distance_layers = []\n",
    "for amen_type in ['bar','restaurant','park','station']:\n",
    "    field = f'avg_dist_to_{amen_type}'\n",
    "    fg = folium.FeatureGroup(name=f\"NTA Avg Distance to {amen_type.title()} (m)\", show=False)\n",
    "    folium.GeoJson(\n",
    "        ntas,\n",
    "        style_function=make_choro_style(ntas[field]),\n",
    "        tooltip=folium.GeoJsonTooltip(\n",
    "            fields=['NTAName', field],\n",
    "            aliases=['Neighborhood', f'Avg m to {amen_type.title()}']\n",
    "        )\n",
    "    ).add_to(fg)\n",
    "    # Numeric labels\n",
    "    for _, r in ntas.iterrows():\n",
    "        val = r[field]\n",
    "        if not np.isnan(val):\n",
    "            folium.map.Marker(\n",
    "                [r.geometry.centroid.y, r.geometry.centroid.x],\n",
    "                icon=folium.DivIcon(html=f\"<div style='font-size:8px;color:black'>{int(val)}</div>\")\n",
    "            ).add_to(fg)\n",
    "    fg.add_to(m)\n",
    "    distance_layers.append(fg)\n",
    "\n",
    "# === 18) Crime→Amenity Distances by Type ===\n",
    "combo_layers = []\n",
    "for crime_cat in ['Felony','Misdemeanor','Violation']:\n",
    "    for amen_type in ['bar','restaurant','park','station']:\n",
    "        field = f'avg_dist_{crime_cat.lower()}_to_{amen_type}'\n",
    "        # Compute mean nearest distance per NTA\n",
    "        subset = crime[crime['LAW_CAT_CD'].str.title() == crime_cat]\n",
    "        c2n_cat = gpd.sjoin(\n",
    "            subset[['geometry', f'dist_to_{amen_type}']],\n",
    "            ntas[['NTA2020','geometry']],\n",
    "            how='inner', predicate='within'\n",
    "        )\n",
    "        avg_dist = c2n_cat.groupby('NTA2020')[f'dist_to_{amen_type}'].mean()\n",
    "        ntas[field] = ntas['NTA2020'].map(avg_dist)\n",
    "\n",
    "        explanation = (\n",
    "            f\"Average over all {crime_cat.lower()} incidents of the \"\n",
    "            f\"distance (in meters) to the single closest {amen_type}\"\n",
    "        )\n",
    "        layer_name = (\n",
    "            f\"NTA {crime_cat}→{amen_type.title()} Distance \"\n",
    "            f\"(*{explanation}*)\"\n",
    "        )\n",
    "        fg = folium.FeatureGroup(name=layer_name, show=False)\n",
    "        folium.GeoJson(\n",
    "            ntas,\n",
    "            style_function=make_choro_style(ntas[field]),\n",
    "            tooltip=folium.GeoJsonTooltip(\n",
    "                fields=['NTAName', field],\n",
    "                aliases=['Neighborhood', f'{crime_cat}→{amen_type.title()} (m)']\n",
    "            )\n",
    "        ).add_to(fg)\n",
    "        # Value labels\n",
    "        for _, r in ntas.iterrows():\n",
    "            val = r[field]\n",
    "            if not np.isnan(val):\n",
    "                folium.map.Marker(\n",
    "                    [r.geometry.centroid.y, r.geometry.centroid.x],\n",
    "                    icon=folium.DivIcon(html=f\"<div style='font-size:7px'>{int(val)}</div>\")\n",
    "                ).add_to(fg)\n",
    "        fg.add_to(m)\n",
    "        combo_layers.append(fg)\n",
    "\n",
    "# === 19) Grouped Layer Control ===\n",
    "GroupedLayerControl(\n",
    "    groups={\n",
    "        'Crime Points':                    crime_point_layers,\n",
    "        'Crime Clusters':                  crime_cluster_layers,\n",
    "        'Crime Heatmap':                   [crime_heat],\n",
    "        'Amenities Points':                amenity_point_layers,\n",
    "        'Amenity Clusters':                amenity_cluster_layers,\n",
    "        'Amenity Heatmap':                 [amen_heat],\n",
    "        'NTA Metrics':                     nta_layers,\n",
    "        'Borough Metrics':                 boro_layers,\n",
    "        'NTA Distances to Amenities':      distance_layers,\n",
    "        'Crime→Amenity Distances by Type': combo_layers\n",
    "    },\n",
    "    exclusive_groups=[],    # allow multiple on/off\n",
    "    collapse=False,\n",
    "    position='topright'\n",
    ").add_to(m)\n",
    "\n",
    "# === 20) Display the map ===\n",
    "m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brugbar tekst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## god kilde\n",
    "\n",
    "Urban crime patterns have been studied from both social and spatial perspectives. Criminology theories such as Broken Windows (Wilson & Kelling, 1982) suggest that the environment plays a role in crime prevalence – for example, disorder in the physical environment might encourage criminal behavior. On the other hand, urbanist Jane Jacobs (1961) argued that active streetscapes with plenty of “eyes on the street” can deter crime, implying that amenities attracting people (cafés, bars, etc.) might enhance safety through informal surveillance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crime Hotspots (Clustering Analysis) - DBSCAN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DBSCAN clustering to identify hotspots ---\n",
    "coords = np.column_stack([crime_gdf.geometry.x, crime_gdf.geometry.y])\n",
    "db = DBSCAN(eps=500, min_samples=30).fit(coords)\n",
    "crime_gdf[\"cluster\"] = db.labels_\n",
    "\n",
    "# Identify top 5 largest clusters (exclude noise)\n",
    "labels = crime_gdf[\"cluster\"].values\n",
    "clusters = [lab for lab in set(labels) if lab != -1]\n",
    "clusters.sort(key=lambda c: (labels == c).sum(), reverse=True)\n",
    "top5 = clusters[:5]\n",
    "print(\"Top 5 clusters and sizes:\", {c: int((labels == c).sum()) for c in top5})\n",
    "\n",
    "# Build convex hulls for these clusters\n",
    "hotspot_hulls = []\n",
    "for c in top5:\n",
    "    pts = crime_gdf[crime_gdf[\"cluster\"] == c]\n",
    "    hull = pts.unary_union.convex_hull\n",
    "    hotspot_hulls.append({\n",
    "        \"cluster\": c,\n",
    "        \"count\": int((labels == c).sum()),\n",
    "        \"geometry\": hull\n",
    "    })\n",
    "hotspots_gdf = gpd.GeoDataFrame(hotspot_hulls, crs=crime_gdf.crs)\n",
    "\n",
    "# Ensure Web Mercator for contextily\n",
    "hotspots_gdf = hotspots_gdf.to_crs(epsg=3857)\n",
    "\n",
    "# --- Plotting ---\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "# sample crime points for context\n",
    "crime_gdf.sample(frac=0.1).plot(ax=ax, color=\"grey\", alpha=0.1, markersize=1)\n",
    "\n",
    "# plot hull boundaries and fills\n",
    "hotspots_gdf.boundary.plot(\n",
    "    ax=ax, color='red', linewidth=2, linestyle='--'\n",
    ")\n",
    "hotspots_gdf.plot(\n",
    "    ax=ax, column='cluster', alpha=0.3, cmap='Set1'\n",
    ")\n",
    "\n",
    "# add basemap with fallback\n",
    "try:\n",
    "    ctx.add_basemap(\n",
    "        ax,\n",
    "        source=ctx.providers.OpenStreetMap.Mapnik,\n",
    "        crs=crime_gdf.crs\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"⚠️ Basemap failed:\", e)\n",
    "\n",
    "ax.set_title(\n",
    "    \"Figure 2. Top 5 Crime Hotspots in NYC (2019)\", fontsize=15\n",
    ")\n",
    "ax.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AGAIN USE NTA's instead of boroughs\n",
    "\n",
    "# 1) Ensure boroughs has the necessary columns\n",
    "boroughs['boro_name'] = boroughs.get('boro_name', boroughs['BoroName'])\n",
    "boroughs['crime_count'] = boroughs['crime_count'].fillna(0).astype(int)\n",
    "\n",
    "# 2) Build Queen contiguity weights and row-standardize\n",
    "w = libpysal.weights.Queen.from_dataframe(boroughs)\n",
    "w.transform = 'R'\n",
    "\n",
    "# 3) Extract crime counts and compute spatial lag (neighbor average)\n",
    "counts = boroughs['crime_count'].values\n",
    "lag = libpysal.weights.lag_spatial(w, counts)\n",
    "\n",
    "# 4) Print a summary table\n",
    "print(f\"{'Borough':<20} {'Crime':>6} {'Neighbor Avg':>14}\")\n",
    "print(\"-\" * 42)\n",
    "for name, cnt, nbr_avg in zip(boroughs['boro_name'], counts, lag):\n",
    "    print(f\"{name:<20} {cnt:6d} {nbr_avg:14.1f}\")\n",
    "\n",
    "# 5) Compute Global Moran's I\n",
    "mi = esda.Moran(counts, w)\n",
    "print(f\"\\nGlobal Moran's I: {mi.I:.3f}\")\n",
    "print(f\"p-value:           {mi.p_sim:.3f}\")\n",
    "\n",
    "# 6) Add the spatial lag back to the GeoDataFrame\n",
    "boroughs['spatial_lag'] = lag\n",
    "\n",
    "# 7) Choropleth maps side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 7))\n",
    "\n",
    "boroughs.plot(\n",
    "    column='crime_count',\n",
    "    cmap='Reds',\n",
    "    legend=True,\n",
    "    ax=axes[0],\n",
    "    edgecolor='black'\n",
    ")\n",
    "axes[0].set_title('Crime Count by Borough (2019)')\n",
    "axes[0].axis('off')\n",
    "\n",
    "boroughs.plot(\n",
    "    column='spatial_lag',\n",
    "    cmap='Blues',\n",
    "    legend=True,\n",
    "    ax=axes[1],\n",
    "    edgecolor='black'\n",
    ")\n",
    "axes[1].set_title('Spatial Lag of Crime Count')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 8) Moran’s I scatterplot\n",
    "y     = (counts      - counts.mean())      / counts.std()\n",
    "y_lag = (lag - lag.mean()) / lag.std()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "ax.axhline(0, color='gray', linewidth=1)\n",
    "ax.axvline(0, color='gray', linewidth=1)\n",
    "ax.scatter(y, y_lag, s=100, color='steelblue')\n",
    "\n",
    "for i, name in enumerate(boroughs['boro_name']):\n",
    "    ax.text(y[i] + 0.02, y_lag[i] + 0.02, name, fontsize=9)\n",
    "\n",
    "ax.set_xlabel('Standardized Crime Count')\n",
    "ax.set_ylabel('Standardized Spatial Lag')\n",
    "ax.set_title(\"Moran's I Scatterplot\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Count restaurants per borough via spatial join\n",
    "rests = amenities_gdf[amenities_gdf[\"category\"] == \"restaurant\"]\n",
    "rests_in_boro = gpd.sjoin(\n",
    "    rests,\n",
    "    boroughs[[\"boro_name\", \"geometry\"]],\n",
    "    how=\"inner\",\n",
    "    predicate=\"within\"\n",
    ")\n",
    "rest_counts = rests_in_boro.groupby(\"boro_name\").size().to_dict()\n",
    "\n",
    "# 2) Add restaurant counts to boroughs GeoDataFrame\n",
    "boroughs[\"rest_count\"] = boroughs[\"boro_name\"].map(rest_counts).fillna(0).astype(int)\n",
    "\n",
    "# 3) Build feature matrix [crime_count, rest_count] and normalize\n",
    "features = boroughs[[\"crime_count\", \"rest_count\"]].values\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(features)\n",
    "\n",
    "# 4) Fit KMeans (2 clusters) and attach labels\n",
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n",
    "boroughs[\"cluster\"] = kmeans.labels_\n",
    "\n",
    "# 5) Print results\n",
    "for _, row in boroughs.iterrows():\n",
    "    print(\n",
    "        f\"{row.boro_name}: Crime={row.crime_count}, \"\n",
    "        f\"Restaurants={row.rest_count}, Cluster={row.cluster}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Prepare coordinates of all crime points (in Web Mercator meters)\n",
    "coords = np.column_stack([crime_gdf.geometry.x, crime_gdf.geometry.y])\n",
    "\n",
    "# 2. Compute nearest‐neighbor distances (skip self–distance)\n",
    "nn = NearestNeighbors(n_neighbors=2, metric='euclidean').fit(coords)\n",
    "distances, _ = nn.kneighbors(coords)\n",
    "nn_distances = distances[:, 1]  # true nearest neighbor\n",
    "\n",
    "# 3. Calculate observed Average Nearest Neighbor (ANN)\n",
    "ann = nn_distances.mean()\n",
    "\n",
    "# 4. Calculate expected ANN under Complete Spatial Randomness (CSR)\n",
    "area = boroughs.geometry.unary_union.area     # total study area in m²\n",
    "n_points = len(crime_gdf)                     # number of crime incidents\n",
    "expected_ann = 0.5 * np.sqrt(area / n_points)\n",
    "\n",
    "# 5. Calculate the R ratio\n",
    "R = ann / expected_ann\n",
    "\n",
    "# 6. Beginner‐friendly printout\n",
    "print(\"=== Average Nearest Neighbor (ANN) Analysis ===\")\n",
    "print(f\"Observed ANN: {ann:.2f} m\")\n",
    "print(f\"Expected ANN (CSR): {expected_ann:.2f} m\")\n",
    "print(f\"R ratio: {R:.3f}\\n\")\n",
    "print(\"Interpretation:\")\n",
    "if R < 1:\n",
    "    print(f\"  • R = {R:.3f} < 1 → points are clustered (closer than random).\")\n",
    "elif R > 1:\n",
    "    print(f\"  • R = {R:.3f} > 1 → points are dispersed (farther than random).\")\n",
    "else:\n",
    "    print(f\"  • R = {R:.3f} ≈ 1 → random spatial pattern.\")\n",
    "print(f\"On average, each crime is {ann:.2f} m from its nearest neighbor,\")\n",
    "print(f\"much smaller than the {expected_ann:.2f} m we'd expect if crimes were random.\\n\")\n",
    "\n",
    "# 7. Visualization: histogram of nearest‐neighbor distances\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(nn_distances, bins=50, edgecolor='black')\n",
    "plt.axvline(ann, color='red', linestyle='--', linewidth=2, label=f'Observed ANN ({ann:.2f} m)')\n",
    "plt.axvline(expected_ann, color='blue', linestyle=':', linewidth=2, label=f'Expected ANN ({expected_ann:.2f} m)')\n",
    "plt.title(\"Histogram of Nearest‐Neighbor Distances (Crime Incidents)\")\n",
    "plt.xlabel(\"Distance to Nearest Crime (meters)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
